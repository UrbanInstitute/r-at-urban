<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="site_libs/d3-3.5.6/d3.min.js"></script>
<link href="site_libs/profvis-0.3.5/profvis.css" rel="stylesheet" />
<script src="site_libs/profvis-0.3.5/profvis.js"></script>
<link href="site_libs/highlight-6.2.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlight-6.2.0/highlight.js"></script>
<script src="site_libs/profvis-binding-0.3.5/profvis.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114294141-1"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js', new Date());

 gtag('config', 'UA-114294141-1');
</script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">R PROGRAMMING AT THE URBAN INSTITUTE</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li>
  <a href="intro-to-r.html">INTRO TO R</a>
</li>
<li>
  <a href="graphics-guide.html">PLOTTING</a>
</li>
<li>
  <a href="mapping.html">MAPPING</a>
</li>
<li>
  <a href="optimization.html">OPTIMIZATION</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<p><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato" /></p>
<div id="header">
<div class="figure">
<img src="optimization/images/urban-institute-logo.png" />

</div>
<hr />
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This guide outlines tools and tips for improving the speed and execution of R code.</p>
<p>Sometimes, simply tweaking a few lines of code can lead to large performance gains in the execution of a program. Other issues may take more time to work through but can be a huge benefit to a project in the long term.</p>
<p>An important lesson to learn when it comes to optimising an R (or any) program is knowing both if to start and when to stop. You most likely want to optimize your code because it is “too slow”, but what that means will vary from project to project. Be sure to consider what “fast enough” is for your project and how much needs to be optimized. If your program takes an hour to complete, spending 5 hours trying to make it faster can be time well spent if the script will be run regularly, and a complete waste of time if it’s an ad-hoc analysis.</p>
<p>For more information, see the CRAN Task View <a href="%22https://CRAN.R-project.org/view=HighPerformanceComputing%22">High-Performance and Parallel Computing with R</a>. The “Performant Code” section of Hadley Wickham’s <a href="%22http://adv-r.had.co.nz/%22">Advanced R</a> is another great resource and provides a deeper dive into what is covered in this guide.</p>
<hr />
</div>
<div id="update-your-installation" class="section level1">
<h1>Update Your Installation</h1>
<p>One of the easiest ways to improve the performance of R is to update R. In general, R will have a big annual release (i.e., 3.5.0) in the spring and around 3-4 smaller patch releases (i.e., 3.5.1) throughout the rest of the year. If the middle digit of your installation is behind the current release, you should consider updating.</p>
<p>For instance, R 3.5.0 implemented an improved read from text files. A 5GB file took over 5 minutes to read in 3.4.4:</p>
<div class="figure">
<img src="optimization/images/data-load-3-4.PNG" style="width:75.0%" />

</div>
<p>While 3.5.0 took less than half the time:</p>
<div class="figure">
<img src="optimization/images/data-load-3-5.PNG" style="width:75.0%" />

</div>
<p>To see what the R-core development team is up to, check out the <a href="%22https://cran.r-project.org/doc/manuals/r-devel/NEWS.html%22">NEWS</a> file from the R project.</p>
<hr />
</div>
<div id="profiling-benchmarking" class="section level1">
<h1>Profiling &amp; Benchmarking</h1>
<p>In order to efficiently optimize your code, you’ll first need to know where it’s running slowest. The <code>profvis</code> package provides a nice way of visualizing the execution time and memory useage of your program.</p>
<pre class="r"><code>library(profvis)
library(dplyr)

profvis({
  diamonds &lt;- read.csv(&quot;optimization/data/diamonds.csv&quot;)
  
  diamonds_by_cut &lt;- diamonds %&gt;%
    group_by(cut) %&gt;%
    summarise_if(is.numeric, mean)

  write.csv(diamonds_by_cut, file = &quot;optimization/data/diamonds_by_cut.csv&quot;)  

})</code></pre>
<div id="htmlwidget-aeb1826a88aa9951cbbe" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-aeb1826a88aa9951cbbe">{"x":{"message":{"prof":{"time":[1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7,8,8,8,9,9,9,9,9,10,10,10,10,10,11,11,11,11,11,12,12,12,12,12,13,13,13,13,13,14,14,14,14,14,15,15,15,15,15,16,16,16,16,16,17,17,17,17,17,18,18,18,18,18,19,19,19,19,19,19,19,19,19,19,19,20,20,20,20,20,20,20],"depth":[3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1],"label":["scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv","grouped_df_impl","grouped_df","group_by.data.frame","group_by","function_list[[i]]","freduce","_fseq","eval","eval","withVisible","%>%","getOption","file","write.table","eval","eval","eval.parent","write.csv"],"filenum":[null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1],"linenum":[null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,null,null,null,null,null,7,null,null,null,null,null,null,11],"memalloc":[8.8349609375,8.8349609375,8.8349609375,10.0664901733398,10.0664901733398,10.0664901733398,12.5238647460938,12.5238647460938,12.5238647460938,12.5519256591797,12.5519256591797,12.5519256591797,13.5411911010742,13.5411911010742,13.5411911010742,17.4493865966797,17.4493865966797,17.4493865966797,17.4538803100586,17.4538803100586,17.4538803100586,17.4603881835938,17.4603881835938,17.4603881835938,21.9973220825195,21.9973220825195,21.9973220825195,21.9973220825195,21.9973220825195,22.7033615112305,22.7033615112305,22.7033615112305,22.7033615112305,22.7033615112305,26.3796997070312,26.3796997070312,26.3796997070312,26.3796997070312,26.3796997070312,26.9972686767578,26.9972686767578,26.9972686767578,26.9972686767578,26.9972686767578,27.2033081054688,27.2033081054688,27.2033081054688,27.2033081054688,27.2033081054688,27.6151123046875,27.6151123046875,27.6151123046875,27.6151123046875,27.6151123046875,27.6151123046875,27.6151123046875,27.6151123046875,27.6151123046875,27.6151123046875,28.0269165039062,28.0269165039062,28.0269165039062,28.0269165039062,28.0269165039062,28.0269165039062,28.0269165039062,28.0269165039062,28.0269165039062,28.0269165039062,28.438720703125,28.438720703125,28.438720703125,28.438720703125,28.438720703125,28.9139175415039,28.9139175415039,28.9139175415039,28.9139175415039,28.9139175415039,28.9139175415039,28.9139175415039,28.9139175415039,28.9139175415039,28.9139175415039,28.9139175415039,31.0554122924805,31.0554122924805,31.0554122924805,31.0554122924805,31.0554122924805,31.0554122924805,31.0554122924805],"meminc":[0,0,0,1.23152923583984,0,0,2.45737457275391,0,0,0.0280609130859375,0,0,0.989265441894531,0,0,3.90819549560547,0,0,0.00449371337890625,0,0,0.00650787353515625,0,0,4.53693389892578,0,0,0,0,0.706039428710938,0,0,0,0,3.67633819580078,0,0,0,0,0.617568969726562,0,0,0,0,0.206039428710938,0,0,0,0,0.41180419921875,0,0,0,0,0,0,0,0,0,0.41180419921875,0,0,0,0,0,0,0,0,0,0.41180419921875,0,0,0,0,0.475196838378906,0,0,0,0,0,0,0,0,0,0,2.14149475097656,0,0,0,0,0,0],"filename":[null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"library(profvis)\nlibrary(dplyr)\n\nprofvis({\n\tdiamonds <- read.csv(\"optimization/data/diamonds.csv\")\n\t\n\tdiamonds_by_cut <- diamonds %>%\n\t\tgroup_by(cut) %>%\n\t\tsummarise_if(is.numeric, mean)\n\n\twrite.csv(diamonds_by_cut, file = \"optimization/data/diamonds_by_cut.csv\")\t\n\n})\n","normpath":"<expr>"}],"prof_output":"/var/folders/r3/3w_52nz53v71pw1srn6nmxdr0000gn/T//RtmpkqwvEa/filee65e2d68d0e2.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
<p>In this toy example it looks like the <code>read.csv</code> function is the bottleneck, so work on optimizing that first.</p>
<p>Once you find the bottleneck that needs to be optimized, it can be useful to benchmark different potential solutions. The <code>microbenchmark</code> package can help you choose between different options. Continuing with the simple example with the <code>diamonds</code> dataset, compare the base <code>read.csv</code> function with <code>read_csv</code> from the <code>readr</code> package.</p>
<pre class="r"><code>library(microbenchmark)

microbenchmark(
  read.csv(&quot;optimization/data/diamonds.csv&quot;),
  readr::read_csv(&quot;optimization/data/diamonds.csv&quot;)
)</code></pre>
<pre><code>## Unit: milliseconds
##                                               expr       min        lq
##         read.csv(&quot;optimization/data/diamonds.csv&quot;) 172.48112 175.68515
##  readr::read_csv(&quot;optimization/data/diamonds.csv&quot;)  74.51706  77.22967
##       mean    median        uq      max neval
##  181.81313 179.13197 181.99616 225.9102   100
##   81.22653  79.11934  81.73556 161.9955   100</code></pre>
<p>In this case, <code>read_csv</code> is about twice as fast as the base R implementations.</p>
</div>
<div id="parallel-computing" class="section level1">
<h1>Parallel Computing</h1>
<p>Often, time-intensive R code can be sped up by breaking the execution of the job across additional cores of your computer. This is called parallel computing.</p>
<div id="learn-lapplypurrrmap" class="section level2">
<h2>Learn <code>lapply</code>/<code>purrr::map</code></h2>
<p>Learning the <code>lapply</code> (and variants) function from Base R or the <code>map</code> (and variants) function from the <code>purrr</code> package is the first step in learning to run R code in parallel. Once you understand how <code>lapply</code> and <code>map</code> work, running your code in parallel will be simple.</p>
<p>Say you have a vector of numbers and want to find the square root of each one (ignore for now that <code>sqrt</code> is vectorized, which will be covered later). You could write a for loop and iterate over each element of the vector:</p>
<pre class="r"><code>x &lt;- c(1, 4, 9, 16)

out &lt;- vector(&quot;list&quot;, length(x))
for (i in seq_along(x)) {
  out[[i]] &lt;- sqrt(x[[i]])
}
unlist(out)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>The <code>lapply</code> function essentially handles the overhead of constructing a for loop for you. The syntax is:</p>
<pre class="r"><code>lapply(X, FUN, ...)</code></pre>
<p><code>lapply</code> will then take each element of <code>X</code> and apply the <code>FUN</code>ction to it. Our simple example then becomes:</p>
<pre class="r"><code>x &lt;- c(1, 4, 9, 16)
out &lt;- lapply(x, sqrt)
unlist(out)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>Those working within the <code>tidyverse</code> may use <code>map</code> from the <code>purrr</code> package equivalently:</p>
<pre class="r"><code>library(purrr)
x &lt;- c(1, 4, 9, 16)
out &lt;- map(x, sqrt)
unlist(out)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
</div>
<div id="motivating-example" class="section level2">
<h2>Motivating Example</h2>
<p>Once you are comfortable with <code>lapply</code> and/or <code>map</code>, running the same code in parallel takes just an additional line of code.</p>
<p>For <code>lapply</code> users, the <code>future.apply</code> package contains an equivalent <code>future_lapply</code> function. Just be sure to call <code>plan(multiprocess)</code> beforehand, which will handle the back-end orchestration needed to run in parallel.</p>
<pre class="r"><code># install.packages(&quot;future.apply&quot;)
library(future.apply)
plan(multiprocess)
out &lt;- future_lapply(x, sqrt)
unlist(out)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>For <code>purrr</code> users, the <code>furrr</code> (i.e., future purrr) package includes an equivalent <code>future_map</code> function:</p>
<pre class="r"><code># install.packages(&quot;furrr&quot;)
library(furrr)
plan(multiprocess)
y &lt;- future_map(x, sqrt)
unlist(y)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>How much faster did this simple example run in parallel?</p>
<pre class="r"><code>library(future.apply)
plan(multiprocess)

x &lt;- c(1, 4, 9, 16)

microbenchmark::microbenchmark(
  sequential = lapply(x, sqrt),
  parallel = future_lapply(x, sqrt),
  unit = &quot;s&quot;
)</code></pre>
<pre><code>## Unit: seconds
##        expr        min           lq         mean      median          uq
##  sequential 0.00000279 0.0000083235 0.0001163979 0.000012938 0.000020489
##    parallel 0.05367924 0.0696213500 0.0734207716 0.072080636 0.075900806
##          max neval
##  0.001748501   100
##  0.101781930   100</code></pre>
<p>Parallelization was actually slower. In this case, the overhead of setting the code to run in parallel far outweighed any performance gain. In general, parallelization works well on long-running &amp; compute intensive jobs.</p>
</div>
<div id="a-somewhat-more-complex-example" class="section level2">
<h2>A (somewhat) More Complex Example</h2>
<p>In this example we’ll use the <code>diamonds</code> dataset from <code>ggplot2</code> and perform a kmeans cluster. We’ll use <code>lapply</code> to iterate the number of clusters from 2 to 5:</p>
<pre class="r"><code>df &lt;- ggplot2::diamonds
df &lt;- dplyr::select(df, -c(cut, color, clarity))

centers = 2:5

system.time(
  lapply(centers, 
         function(x) kmeans(df, centers = x, nstart = 500)
         )
  )</code></pre>
<pre><code>##    user  system elapsed 
##  48.924   2.368  51.509</code></pre>
<p>A now running the same code in parallel:</p>
<pre class="r"><code>library(future.apply)
plan(multiprocess)

system.time(
  future_lapply(centers, 
                function(x) kmeans(df, centers = x, nstart = 500)
                )
  )</code></pre>
<pre><code>##    user  system elapsed 
##   0.122   0.052  32.892</code></pre>
<p>While we didn’t achieve perfect scaling, we still get a nice bump in execution time.</p>
</div>
<div id="additional-packages" class="section level2">
<h2>Additional Packages</h2>
<p>For the sake of ease and brevity, this guide focused on the <code>futures</code> framework for parallelization. However, you should be aware that there are a number of other ways to parallelize your code.</p>
<div id="the-parallel-package" class="section level3">
<h3>The <code>parallel</code> Package</h3>
<p>The <code>parallel</code> package is included in your base R installation. It includes analogues of the various <code>apply</code> functions:</p>
<ul>
<li><code>parLapply</code></li>
<li><code>mclapply</code> - not available on Windows</li>
</ul>
<p>These functions generally require more setup, especially on Windows machines.</p>
</div>
<div id="the-doparallel-package" class="section level3">
<h3>The <code>doParallel</code> Package</h3>
<p>The <code>doParallel</code> package builds off of <code>parallel</code> and is useful for code that uses for loops instead of <code>lapply</code>. Like the parallel package, it generally requires more setup, especially on Windows machines.</p>
</div>
<div id="machine-learning---caret" class="section level3">
<h3>Machine Learning - <code>caret</code></h3>
<p>For those running machine learning models, the <code>caret</code> package can easily leverage <code>doParallel</code> to speed up the execution of multiple models. Lifting the example from the package documentation:</p>
<pre class="r"><code>library(doParallel)
cl &lt;- makePSOCKcluster(5) # number of cores to use
registerDoParallel(cl)

## All subsequent models are then run in parallel
model &lt;- train(y ~ ., data = training, method = &quot;rf&quot;)

## When you are done:
stopCluster(cl)</code></pre>
<p>Be sure to check out the full <a href="%22http://topepo.github.io/caret/parallel-processing.html%22">documentation</a> for more detail.</p>
<hr />
</div>
</div>
</div>
<div id="big-data" class="section level1">
<h1>Big Data</h1>
<p>As data collection and storage becomes easier and cheaper, it is relatively simple to obtain relatively large data files. An important point to keep in mind is that the size of your data will generally expand when it is read from a storage device into R. A general rule of thumb is that a file will take somewhere around 3-4 times more space in memory than it does on disk.</p>
<p>For instance, compare the size of the <code>iris</code> data set when it is saved as a .csv file locally vs the size of the object when it is read in to an R session:</p>
<pre class="r"><code>file.size(&quot;optimization/iris.csv&quot;) / 1000</code></pre>
<pre><code>## [1] NA</code></pre>
<pre class="r"><code>df &lt;- readr::read_csv(&quot;optimization/data/iris.csv&quot;)
pryr::object_size(df)</code></pre>
<pre><code>## 9.7 kB</code></pre>
<p>This means that on a standard Urban Institute desktop, you may have issues reading in files that are larger than 4 GB.</p>
<div id="object-size" class="section level2">
<h2>Object Size</h2>
<p>The type of your data can have a big impact on the size of your data frame when you are dealing with larger files. There are four main types of atomic vectors in R:</p>
<ol style="list-style-type: decimal">
<li><code>logical</code></li>
<li><code>integer</code></li>
<li><code>double</code> (also called <code>numeric</code>)</li>
<li><code>character</code></li>
</ol>
<p>Each of these data types occupies a different amount of space in memory - <code>logical</code> and <code>integer</code> vectors use 4 bytes per element, while a <code>double</code> will occupy 8 bytes. R uses a global string pool, so <code>character</code> vectors are hard to estimate, but will generally take up more space for element.</p>
<p>Consider the following example:</p>
<pre class="r"><code>x &lt;- 1:100
pryr::object_size(x)</code></pre>
<pre><code>## 448 B</code></pre>
<pre class="r"><code>pryr::object_size(as.double(x))</code></pre>
<pre><code>## 848 B</code></pre>
<pre class="r"><code>pryr::object_size(as.character(x))</code></pre>
<pre><code>## 6.45 kB</code></pre>
<p>An incorrect data type can easily cost you a lot of space in memory, especially at scale. This often happens when reading data from a text or csv file - data may have a format such as <code>c(1.0, 2.0, 3.0)</code> and will be read in as a <code>numeric</code> column, when <code>integer</code> is more appropriate and compact.</p>
<p>You may also be familiar with <code>factor</code> variables within R. Essentially a <code>factor</code> will represent your data as integers, and map them back to their character representation. This can save memory when you have a compact and unique level of factors:</p>
<pre class="r"><code>x &lt;- sample(letters, 10000, replace = T)
pryr::object_size(as.character(x))</code></pre>
<pre><code>## 81.5 kB</code></pre>
<pre class="r"><code>pryr::object_size(as.factor(x))</code></pre>
<pre><code>## 42.1 kB</code></pre>
<p>However if each element is unique, or if there is not a lot of overlap among elements, than the overhead will make a factor larger than its character representation:</p>
<pre class="r"><code>pryr::object_size(as.factor(letters))</code></pre>
<pre><code>## 2.22 kB</code></pre>
<pre class="r"><code>pryr::object_size(as.character(letters))</code></pre>
<pre><code>## 1.71 kB</code></pre>
</div>
<div id="cloud-computing" class="section level2">
<h2>Cloud Computing</h2>
<p>Sometimes, you will have data that are simply too large to ever fit on your local desktop machine. If that is the case, then the Elastic Cloud Computing Environment from the Office of Technology and Data Science can provide you with easy access to powerful analytic tools for computationally intensive project.</p>
<p>The Elastic Cloud Computing Environment allows researchers to quickly spin-up an Amazon Web Services (AWS) Elastic Cloud Compute (EC2) instance. These instances offer increased memory to read in large datasets, along with additional CPUs to provide the ability to process data in parallel at an impressive scale.</p>
<table>
<thead>
<tr class="header">
<th>Instance</th>
<th>CPU</th>
<th>Memory (GB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Desktop</td>
<td>8</td>
<td>16</td>
</tr>
<tr class="even">
<td>c5.4xlarge</td>
<td>16</td>
<td>32</td>
</tr>
<tr class="odd">
<td>c5.9xlarge</td>
<td>36</td>
<td>72</td>
</tr>
<tr class="even">
<td>c5.18xlarge</td>
<td>72</td>
<td>144</td>
</tr>
<tr class="odd">
<td>x1e.8xlarge</td>
<td>32</td>
<td>976</td>
</tr>
<tr class="even">
<td>x1e.16xlarge</td>
<td>64</td>
<td>1952</td>
</tr>
</tbody>
</table>
<p>Feel free to contact Kyle Ueyama (<a href="mailto:kueyama@urban.org">kueyama@urban.org</a>) if this would be useful for your project.</p>
<hr />
</div>
</div>
<div id="common-pitfalls" class="section level1">
<h1>Common Pitfalls</h1>
<div id="for-loops-and-vector-allocation" class="section level2">
<h2>For Loops and Vector Allocation</h2>
<p>A refrain you will often hear is that for loops in R are slow and need to be avoided at all costs. This is not true! Rather, an improperly constructed loop in R can bring the execution of your program to a near standstill.</p>
<p>A common for loop structure may look something like:</p>
<pre class="r"><code>x &lt;- 1:100
out &lt;- c()
for (i in x) {
  out &lt;- c(out, sqrt(x))
  }</code></pre>
<p>The bottleneck in this loop is with the allocation of the vector <code>out</code>. Every time we iterate over an item in <code>x</code> and append it to <code>out</code>, R makes a copy of all the items already in <code>out</code>. As the size of the loop grows, your code will take longer and longer to run.</p>
<p>A better practice is to pre-allocate <code>out</code> to be the correct length, and then insert the results as the loop runs.</p>
<pre class="r"><code>x &lt;- 1:100
out &lt;- rep(NA, length(x))
for (i in seq_along(x)) {
    out[i] &lt;- sqrt(x[i])
}</code></pre>
<p>A quick benchmark shows how much more efficient a loop with a pre-allocated results vector is:</p>
<pre class="r"><code>bad_loop &lt;- function(x) {
  out &lt;- c()
  for (i in x) {
    out &lt;- c(out, sqrt(x))
  }
}

good_loop &lt;- function(x) {
  out &lt;- rep(NA, length(x))
  for (i in seq_along(x)) {
    out[i] &lt;- sqrt(x[i])
  }
}

x &lt;- 1:100
microbenchmark::microbenchmark(
  bad_loop(x),
  good_loop(x)
)</code></pre>
<pre><code>## Unit: microseconds
##          expr      min       lq       mean   median       uq       max
##   bad_loop(x) 1155.727 1382.235 2463.72239 1742.537 2528.570 11719.027
##  good_loop(x)   11.136   11.719   52.72802   15.541   22.142  3549.095
##  neval
##    100
##    100</code></pre>
<p>And note how performance of the “bad” loop degrades as the loop size grows.</p>
<pre class="r"><code>y &lt;- 1:250

microbenchmark::microbenchmark(
  bad_loop(y),
  good_loop(y)
)</code></pre>
<pre><code>## Unit: microseconds
##          expr       min        lq        mean    median        uq
##   bad_loop(y) 49011.292 50273.489 52494.10989 50940.083 52815.690
##  good_loop(y)    25.629    27.093    36.68195    42.183    43.098
##        max neval
##  121806.25   100
##      71.38   100</code></pre>
</div>
<div id="vectorized-functions" class="section level2">
<h2>Vectorized Functions</h2>
<p>Many functions in R are vectorized, meaning they can accept an entire vector (and not just a single value) as input. The <code>sqrt</code> function from the prior examples is one:</p>
<pre class="r"><code>x &lt;- c(1, 4, 9, 16)
sqrt(x)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>This removes the need to use <code>lapply</code> or a for loop. Vectorized functions in R are generally written in a compiled language like C, C++, or FORTRAN, which makes their implementation faster.</p>
<pre class="r"><code>x &lt;- 1:100
microbenchmark::microbenchmark(
  lapply(x, sqrt),
  sqrt(x)
)</code></pre>
<pre><code>## Unit: nanoseconds
##             expr   min    lq     mean  median      uq   max neval
##  lapply(x, sqrt) 32249 32738 35495.74 33268.5 38697.0 66932   100
##          sqrt(x)   820   891  1031.16   936.0  1065.5  3960   100</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
