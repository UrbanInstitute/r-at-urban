<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="site_libs/d3-3.5.6/d3.min.js"></script>
<link href="site_libs/profvis-0.3.5/profvis.css" rel="stylesheet" />
<script src="site_libs/profvis-0.3.5/profvis.js"></script>
<link href="site_libs/highlight-6.2.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlight-6.2.0/highlight.js"></script>
<script src="site_libs/profvis-binding-0.3.5/profvis.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114294141-1"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js', new Date());

 gtag('config', 'UA-114294141-1');
</script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<div class="container-fluid main-container">

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">R PROGRAMMING AT THE URBAN INSTITUTE</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li>
  <a href="intro-to-r.html">INTRO TO R</a>
</li>
<li>
  <a href="graphics-guide.html">PLOTTING</a>
</li>
<li>
  <a href="mapping.html">MAPPING</a>
</li>
<li>
  <a href="optimization.html">OPTIMIZATION</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<p><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato" /></p>
<div id="header">
<div class="figure">
<img src="optimization/images/urban-institute-logo.png" />

</div>
<hr />
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This guide outlines tools and tips for improving the speed and execution of R code.</p>
<p>Sometimes, simply tweaking a few lines of code can lead to large performance gains in the execution of a program. Other issues may take more time to work through but can be a huge benefit to a project in the long term.</p>
<p>An important lesson to learn when it comes to optimising an R (or any) program is knowing both if to start and when to stop. You most likely want to optimize your code because it is “too slow”, but what that means will vary from project to project. Be sure to consider what “fast enough” is for your project and how much needs to be optimized. If your program takes an hour to complete, spending 5 hours trying to make it faster can be time well spent if the script will be run regularly, and a complete waste of time if it’s an ad-hoc analysis.</p>
<p>For more information, see the CRAN Task View <a href="%22https://CRAN.R-project.org/view=HighPerformanceComputing%22">High-Performance and Parallel Computing with R</a>. The “Performant Code” section of Hadley Wickham’s <a href="%22http://adv-r.had.co.nz/%22">Advanced R</a> is another great resource and provides a deeper dive into what is covered in this guide.</p>
<hr />
</div>
<div id="update-your-installation" class="section level1">
<h1>Update Your Installation</h1>
<p>One of the easiest ways to improve the performance of R is to update R. In general, R will have a big annual release (i.e., 3.5.0) in the spring and around 3-4 smaller patch releases (i.e., 3.5.1) throughout the rest of the year. If the middle digit of your installation is behind the current release, you should consider updating.</p>
<p>For instance, R 3.5.0 implemented an improved read from text files. A 5GB file took over 5 minutes to read in 3.4.4:</p>
<div class="figure">
<img src="optimization/images/data-load-3-4.png" style="width:75.0%" />

</div>
<p>While 3.5.0 took less than half the time:</p>
<div class="figure">
<img src="optimization/images/data-load-3-5.png" style="width:75.0%" />

</div>
<p>To see what the R-core development team is up to, check out the <a href="%22https://cran.r-project.org/doc/manuals/r-devel/NEWS.html%22">NEWS</a> file from the R project.</p>
<hr />
</div>
<div id="profiling-benchmarking" class="section level1">
<h1>Profiling &amp; Benchmarking</h1>
<p>In order to efficiently optimize your code, you’ll first need to know where it’s running slowest. The <code>profvis</code> package provides a nice way of visualizing the execution time and memory useage of your program.</p>
<pre class="r"><code>library(profvis)
library(dplyr)

profvis({
  diamonds &lt;- read.csv(&quot;optimization/data/diamonds.csv&quot;)
  
  diamonds_by_cut &lt;- diamonds %&gt;%
    group_by(cut) %&gt;%
    summarise_if(is.numeric, mean)

  write.csv(diamonds_by_cut, file = &quot;optimization/data/diamonds_by_cut.csv&quot;)  

})</code></pre>
<div id="htmlwidget-8b456f2c311110f71d08" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-8b456f2c311110f71d08">{"x":{"message":{"prof":{"time":[1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7,8,8,8,9,9,9,10,10,10,11,11,11,12,12,12,13,13,13,13,13,14,14,14,14,14,15,15,15,15,15,16,16,16,16,16,17,17,17,17,17,18,18,18,18,18,19,19,19,19,19,20,20,20,20,20,20,20,20,20,20,20,20,21,21,21,21,21,21,21],"depth":[3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1],"label":["scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv","scan","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv",".External2","type.convert.default","type.convert","read.table","read.csv","summarise_impl","summarise.tbl_df","summarise","summarise_if","function_list[[k]]","withVisible","freduce","_fseq","eval","eval","withVisible","%>%","close.connection","close","write.table","eval","eval","eval.parent","write.csv"],"filenum":[null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1],"linenum":[null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,null,null,null,null,null,null,7,null,null,null,null,null,null,11],"memalloc":[95.1047897338867,95.1047897338867,95.1047897338867,96.1844940185547,96.1844940185547,96.1844940185547,97.4150238037109,97.4150238037109,97.4150238037109,99.8676681518555,99.8676681518555,99.8676681518555,99.8824691772461,99.8824691772461,99.8824691772461,99.9040222167969,99.9040222167969,99.9040222167969,101.376708984375,101.376708984375,101.376708984375,104.795989990234,104.795989990234,104.795989990234,104.797790527344,104.797790527344,104.797790527344,104.801071166992,104.801071166992,104.801071166992,104.804962158203,104.804962158203,104.804962158203,105.632347106934,105.632347106934,105.632347106934,109.336868286133,109.336868286133,109.336868286133,109.336868286133,109.336868286133,113.307434082031,113.307434082031,113.307434082031,113.307434082031,113.307434082031,114.336814880371,114.336814880371,114.336814880371,114.336814880371,114.336814880371,114.542854309082,114.542854309082,114.542854309082,114.542854309082,114.542854309082,114.954658508301,114.954658508301,114.954658508301,114.954658508301,114.954658508301,115.36646270752,115.36646270752,115.36646270752,115.36646270752,115.36646270752,115.778266906738,115.778266906738,115.778266906738,115.778266906738,115.778266906738,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.427551269531,117.567276000977,117.567276000977,117.567276000977,117.567276000977,117.567276000977,117.567276000977,117.567276000977],"meminc":[0,0,0,1.07970428466797,0,0,1.23052978515625,0,0,2.45264434814453,0,0,0.014801025390625,0,0,0.0215530395507812,0,0,1.47268676757812,0,0,3.41928100585938,0,0,0.001800537109375,0,0,0.0032806396484375,0,0,0.0038909912109375,0,0,0.827384948730469,0,0,3.70452117919922,0,0,0,0,3.97056579589844,0,0,0,0,1.02938079833984,0,0,0,0,0.206039428710938,0,0,0,0,0.41180419921875,0,0,0,0,0.41180419921875,0,0,0,0,0.41180419921875,0,0,0,0,1.64928436279297,0,0,0,0,0,0,0,0,0,0,0,0.139724731445312,0,0,0,0,0,0],"filename":[null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"library(profvis)\nlibrary(dplyr)\n\nprofvis({\n\tdiamonds <- read.csv(\"optimization/data/diamonds.csv\")\n\t\n\tdiamonds_by_cut <- diamonds %>%\n\t\tgroup_by(cut) %>%\n\t\tsummarise_if(is.numeric, mean)\n\n\twrite.csv(diamonds_by_cut, file = \"optimization/data/diamonds_by_cut.csv\")\t\n\n})\n","normpath":"<expr>"}],"prof_output":"D:\\Users\\AWILLI~1\\AppData\\Local\\Temp\\RtmpgZwfrS\\file22203a732baf.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
<p>In this toy example it looks like the <code>read.csv</code> function is the bottleneck, so work on optimizing that first.</p>
<p>Once you find the bottleneck that needs to be optimized, it can be useful to benchmark different potential solutions. The <code>microbenchmark</code> package can help you choose between different options. Continuing with the simple example with the <code>diamonds</code> dataset, compare the base <code>read.csv</code> function with <code>read_csv</code> from the <code>readr</code> package.</p>
<pre class="r"><code>library(microbenchmark)

microbenchmark(
  read.csv(&quot;optimization/data/diamonds.csv&quot;),
  readr::read_csv(&quot;optimization/data/diamonds.csv&quot;)
)</code></pre>
<pre><code>## Unit: milliseconds
##                                               expr      min        lq
##         read.csv(&quot;optimization/data/diamonds.csv&quot;) 155.6819 179.90399
##  readr::read_csv(&quot;optimization/data/diamonds.csv&quot;)  63.0492  75.99749
##       mean    median        uq      max neval cld
##  196.40302 190.65691 204.76792 495.9757   100   b
##   84.98993  82.51298  92.89815 114.3342   100  a</code></pre>
<p>In this case, <code>read_csv</code> is about twice as fast as the base R implementations.</p>
</div>
<div id="parallel-computing" class="section level1">
<h1>Parallel Computing</h1>
<p>Often, time-intensive R code can be sped up by breaking the execution of the job across additional cores of your computer. This is called parallel computing.</p>
<div id="learn-lapplypurrrmap" class="section level2">
<h2>Learn <code>lapply</code>/<code>purrr::map</code></h2>
<p>Learning the <code>lapply</code> (and variants) function from Base R or the <code>map</code> (and variants) function from the <code>purrr</code> package is the first step in learning to run R code in parallel. Once you understand how <code>lapply</code> and <code>map</code> work, running your code in parallel will be simple.</p>
<p>Say you have a vector of numbers and want to find the square root of each one (ignore for now that <code>sqrt</code> is vectorized, which will be covered later). You could write a for loop and iterate over each element of the vector:</p>
<pre class="r"><code>x &lt;- c(1, 4, 9, 16)

out &lt;- vector(&quot;list&quot;, length(x))
for (i in seq_along(x)) {
  out[[i]] &lt;- sqrt(x[[i]])
}
unlist(out)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>The <code>lapply</code> function essentially handles the overhead of constructing a for loop for you. The syntax is:</p>
<pre class="r"><code>lapply(X, FUN, ...)</code></pre>
<p><code>lapply</code> will then take each element of <code>X</code> and apply the <code>FUN</code>ction to it. Our simple example then becomes:</p>
<pre class="r"><code>x &lt;- c(1, 4, 9, 16)
out &lt;- lapply(x, sqrt)
unlist(out)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>Those working within the <code>tidyverse</code> may use <code>map</code> from the <code>purrr</code> package equivalently:</p>
<pre class="r"><code>library(purrr)
x &lt;- c(1, 4, 9, 16)
out &lt;- map(x, sqrt)
unlist(out)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
</div>
<div id="motivating-example" class="section level2">
<h2>Motivating Example</h2>
<p>Once you are comfortable with <code>lapply</code> and/or <code>map</code>, running the same code in parallel takes just an additional line of code.</p>
<p>For <code>lapply</code> users, the <code>future.apply</code> package contains an equivalent <code>future_lapply</code> function. Just be sure to call <code>plan(multiprocess)</code> beforehand, which will handle the back-end orchestration needed to run in parallel.</p>
<pre class="r"><code># install.packages(&quot;future.apply&quot;)
library(future.apply)
plan(multiprocess)
out &lt;- future_lapply(x, sqrt)
unlist(out)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>For <code>purrr</code> users, the <code>furrr</code> (i.e., future purrr) package includes an equivalent <code>future_map</code> function:</p>
<pre class="r"><code># install.packages(&quot;furrr&quot;)
library(furrr)
plan(multiprocess)
y &lt;- future_map(x, sqrt)
unlist(y)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>How much faster did this simple example run in parallel?</p>
<pre class="r"><code>library(future.apply)
plan(multiprocess)

x &lt;- c(1, 4, 9, 16)

microbenchmark::microbenchmark(
  sequential = lapply(x, sqrt),
  parallel = future_lapply(x, sqrt),
  unit = &quot;s&quot;
)</code></pre>
<pre><code>## Unit: seconds
##        expr         min          lq          mean      median          uq
##  sequential 0.000002643 0.000003021 0.00000606048 0.000006042 0.000007363
##    parallel 0.034700694 0.041135382 0.04568158684 0.044017353 0.049636686
##          max neval cld
##  0.000021145   100  a 
##  0.078771473   100   b</code></pre>
<p>Parallelization was actually slower. In this case, the overhead of setting the code to run in parallel far outweighed any performance gain. In general, parallelization works well on long-running &amp; compute intensive jobs.</p>
</div>
<div id="a-somewhat-more-complex-example" class="section level2">
<h2>A (somewhat) More Complex Example</h2>
<p>In this example we’ll use the <code>diamonds</code> dataset from <code>ggplot2</code> and perform a kmeans cluster. We’ll use <code>lapply</code> to iterate the number of clusters from 2 to 5:</p>
<pre class="r"><code>df &lt;- ggplot2::diamonds
df &lt;- dplyr::select(df, -c(cut, color, clarity))

centers = 2:5

system.time(
  lapply(centers, 
         function(x) kmeans(df, centers = x, nstart = 500)
         )
  )</code></pre>
<pre><code>##    user  system elapsed 
##   51.38    2.05   53.71</code></pre>
<p>A now running the same code in parallel:</p>
<pre class="r"><code>library(future.apply)
plan(multiprocess)

system.time(
  future_lapply(centers, 
                function(x) kmeans(df, centers = x, nstart = 500)
                )
  )</code></pre>
<pre><code>##    user  system elapsed 
##    0.10    0.17   31.06</code></pre>
<p>While we didn’t achieve perfect scaling, we still get a nice bump in execution time.</p>
</div>
<div id="additional-packages" class="section level2">
<h2>Additional Packages</h2>
<p>For the sake of ease and brevity, this guide focused on the <code>futures</code> framework for parallelization. However, you should be aware that there are a number of other ways to parallelize your code.</p>
<div id="the-parallel-package" class="section level3">
<h3>The <code>parallel</code> Package</h3>
<p>The <code>parallel</code> package is included in your base R installation. It includes analogues of the various <code>apply</code> functions:</p>
<ul>
<li><code>parLapply</code></li>
<li><code>mclapply</code> - not available on Windows</li>
</ul>
<p>These functions generally require more setup, especially on Windows machines.</p>
</div>
<div id="the-doparallel-package" class="section level3">
<h3>The <code>doParallel</code> Package</h3>
<p>The <code>doParallel</code> package builds off of <code>parallel</code> and is useful for code that uses for loops instead of <code>lapply</code>. Like the parallel package, it generally requires more setup, especially on Windows machines.</p>
</div>
<div id="machine-learning---caret" class="section level3">
<h3>Machine Learning - <code>caret</code></h3>
<p>For those running machine learning models, the <code>caret</code> package can easily leverage <code>doParallel</code> to speed up the execution of multiple models. Lifting the example from the package documentation:</p>
<pre class="r"><code>library(doParallel)
cl &lt;- makePSOCKcluster(5) # number of cores to use
registerDoParallel(cl)

## All subsequent models are then run in parallel
model &lt;- train(y ~ ., data = training, method = &quot;rf&quot;)

## When you are done:
stopCluster(cl)</code></pre>
<p>Be sure to check out the full <a href="%22http://topepo.github.io/caret/parallel-processing.html%22">documentation</a> for more detail.</p>
<hr />
</div>
</div>
</div>
<div id="big-data" class="section level1">
<h1>Big Data</h1>
<p>As data collection and storage becomes easier and cheaper, it is relatively simple to obtain relatively large data files. An important point to keep in mind is that the size of your data will generally expand when it is read from a storage device into R. A general rule of thumb is that a file will take somewhere around 3-4 times more space in memory than it does on disk.</p>
<p>For instance, compare the size of the <code>iris</code> data set when it is saved as a .csv file locally vs the size of the object when it is read in to an R session:</p>
<pre class="r"><code>file.size(&quot;optimization/data/iris.csv&quot;) / 1000</code></pre>
<pre><code>## [1] 3.867</code></pre>
<pre class="r"><code>df &lt;- readr::read_csv(&quot;optimization/data/iris.csv&quot;)
pryr::object_size(df)</code></pre>
<pre><code>## 9.91 kB</code></pre>
<p>This means that on a standard Urban Institute desktop, you may have issues reading in files that are larger than 4 GB.</p>
<div id="object-size" class="section level2">
<h2>Object Size</h2>
<p>The type of your data can have a big impact on the size of your data frame when you are dealing with larger files. There are four main types of atomic vectors in R:</p>
<ol style="list-style-type: decimal">
<li><code>logical</code></li>
<li><code>integer</code></li>
<li><code>double</code> (also called <code>numeric</code>)</li>
<li><code>character</code></li>
</ol>
<p>Each of these data types occupies a different amount of space in memory - <code>logical</code> and <code>integer</code> vectors use 4 bytes per element, while a <code>double</code> will occupy 8 bytes. R uses a global string pool, so <code>character</code> vectors are hard to estimate, but will generally take up more space for element.</p>
<p>Consider the following example:</p>
<pre class="r"><code>x &lt;- 1:100
pryr::object_size(x)</code></pre>
<pre><code>## 448 B</code></pre>
<pre class="r"><code>pryr::object_size(as.double(x))</code></pre>
<pre><code>## 848 B</code></pre>
<pre class="r"><code>pryr::object_size(as.character(x))</code></pre>
<pre><code>## 6.45 kB</code></pre>
<p>An incorrect data type can easily cost you a lot of space in memory, especially at scale. This often happens when reading data from a text or csv file - data may have a format such as <code>c(1.0, 2.0, 3.0)</code> and will be read in as a <code>numeric</code> column, when <code>integer</code> is more appropriate and compact.</p>
<p>You may also be familiar with <code>factor</code> variables within R. Essentially a <code>factor</code> will represent your data as integers, and map them back to their character representation. This can save memory when you have a compact and unique level of factors:</p>
<pre class="r"><code>x &lt;- sample(letters, 10000, replace = TRUE)
pryr::object_size(as.character(x))</code></pre>
<pre><code>## 81.5 kB</code></pre>
<pre class="r"><code>pryr::object_size(as.factor(x))</code></pre>
<pre><code>## 42.1 kB</code></pre>
<p>However if each element is unique, or if there is not a lot of overlap among elements, than the overhead will make a factor larger than its character representation:</p>
<pre class="r"><code>pryr::object_size(as.factor(letters))</code></pre>
<pre><code>## 2.22 kB</code></pre>
<pre class="r"><code>pryr::object_size(as.character(letters))</code></pre>
<pre><code>## 1.71 kB</code></pre>
</div>
<div id="cloud-computing" class="section level2">
<h2>Cloud Computing</h2>
<p>Sometimes, you will have data that are simply too large to ever fit on your local desktop machine. If that is the case, then the Elastic Cloud Computing Environment from the Office of Technology and Data Science can provide you with easy access to powerful analytic tools for computationally intensive project.</p>
<p>The Elastic Cloud Computing Environment allows researchers to quickly spin-up an Amazon Web Services (AWS) Elastic Cloud Compute (EC2) instance. These instances offer increased memory to read in large datasets, along with additional CPUs to provide the ability to process data in parallel at an impressive scale.</p>
<table>
<thead>
<tr class="header">
<th>Instance</th>
<th>CPU</th>
<th>Memory (GB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Desktop</td>
<td>8</td>
<td>16</td>
</tr>
<tr class="even">
<td>c5.4xlarge</td>
<td>16</td>
<td>32</td>
</tr>
<tr class="odd">
<td>c5.9xlarge</td>
<td>36</td>
<td>72</td>
</tr>
<tr class="even">
<td>c5.18xlarge</td>
<td>72</td>
<td>144</td>
</tr>
<tr class="odd">
<td>x1e.8xlarge</td>
<td>32</td>
<td>976</td>
</tr>
<tr class="even">
<td>x1e.16xlarge</td>
<td>64</td>
<td>1952</td>
</tr>
</tbody>
</table>
<p>Feel free to contact Kyle Ueyama (<a href="mailto:kueyama@urban.org">kueyama@urban.org</a>) if this would be useful for your project.</p>
<hr />
</div>
</div>
<div id="common-pitfalls" class="section level1">
<h1>Common Pitfalls</h1>
<div id="for-loops-and-vector-allocation" class="section level2">
<h2>For Loops and Vector Allocation</h2>
<p>A refrain you will often hear is that for loops in R are slow and need to be avoided at all costs. This is not true! Rather, an improperly constructed loop in R can bring the execution of your program to a near standstill.</p>
<p>A common for loop structure may look something like:</p>
<pre class="r"><code>x &lt;- 1:100
out &lt;- c()
for (i in x) {
  out &lt;- c(out, sqrt(x))
  }</code></pre>
<p>The bottleneck in this loop is with the allocation of the vector <code>out</code>. Every time we iterate over an item in <code>x</code> and append it to <code>out</code>, R makes a copy of all the items already in <code>out</code>. As the size of the loop grows, your code will take longer and longer to run.</p>
<p>A better practice is to pre-allocate <code>out</code> to be the correct length, and then insert the results as the loop runs.</p>
<pre class="r"><code>x &lt;- 1:100
out &lt;- rep(NA, length(x))
for (i in seq_along(x)) {
    out[i] &lt;- sqrt(x[i])
}</code></pre>
<p>A quick benchmark shows how much more efficient a loop with a pre-allocated results vector is:</p>
<pre class="r"><code>bad_loop &lt;- function(x) {
  out &lt;- c()
  for (i in x) {
    out &lt;- c(out, sqrt(x))
  }
}

good_loop &lt;- function(x) {
  out &lt;- rep(NA, length(x))
  for (i in seq_along(x)) {
    out[i] &lt;- sqrt(x[i])
  }
}

x &lt;- 1:100
microbenchmark::microbenchmark(
  bad_loop(x),
  good_loop(x)
)</code></pre>
<pre><code>## Unit: microseconds
##          expr     min        lq       mean   median       uq       max
##   bad_loop(x) 989.982 1072.6695 1710.92762 1246.539 1349.426 16203.298
##  good_loop(x)   8.685   10.1945   79.61062   12.649   18.501  6466.217
##  neval cld
##    100   b
##    100  a</code></pre>
<p>And note how performance of the “bad” loop degrades as the loop size grows.</p>
<pre class="r"><code>y &lt;- 1:250

microbenchmark::microbenchmark(
  bad_loop(y),
  good_loop(y)
)</code></pre>
<pre><code>## Unit: microseconds
##          expr       min        lq        mean    median         uq
##   bad_loop(y) 18149.657 28343.217 36582.41693 33185.894 39934.9080
##  good_loop(y)    21.145    26.619    44.64039    41.156    53.4265
##         max neval cld
##  279785.618   100   b
##     111.005   100  a</code></pre>
</div>
<div id="vectorized-functions" class="section level2">
<h2>Vectorized Functions</h2>
<p>Many functions in R are vectorized, meaning they can accept an entire vector (and not just a single value) as input. The <code>sqrt</code> function from the prior examples is one:</p>
<pre class="r"><code>x &lt;- c(1, 4, 9, 16)
sqrt(x)</code></pre>
<pre><code>## [1] 1 2 3 4</code></pre>
<p>This removes the need to use <code>lapply</code> or a for loop. Vectorized functions in R are generally written in a compiled language like C, C++, or FORTRAN, which makes their implementation faster.</p>
<pre class="r"><code>x &lt;- 1:100
microbenchmark::microbenchmark(
  lapply(x, sqrt),
  sqrt(x)
)</code></pre>
<pre><code>## Unit: microseconds
##             expr    min      lq     mean   median       uq     max neval
##  lapply(x, sqrt) 47.574 62.1105 95.83085 101.3775 113.2710 180.478   100
##          sqrt(x)  1.133  2.0775  4.02162   3.3985   4.3425  34.359   100
##  cld
##    b
##   a</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
