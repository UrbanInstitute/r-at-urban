{"title":"Introduction","markdown":{"yaml":{"output":{"html_document":{"includes":{"in_header":"analytics.html"},"css":"styles.css","toc":true,"toc_float":true,"pandoc_args":"--tab-stop=2"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n<link rel=\"stylesheet\" href=\"//fonts.googleapis.com/css?family=Lato\" />\n\n<div id=\"header\">\n<img src=\"graphics-guide/www/images/urban-institute-logo.png\" width=\"350\">\n</div>\n\n```{r markdown-setup, include=FALSE}\nknitr::opts_chunk$set(message = FALSE)\nknitr::opts_chunk$set(echo = TRUE)\nknitr::opts_chunk$set(warning = FALSE)\n\noptions(scipen = 999)\n```\n\n------\n\n\nThis guide outlines tools and tips for improving the speed and execution of R code. \n\nSometimes, simply tweaking a few lines of code can lead to large performance \ngains in the execution of a program. Other issues may take more time to work \nthrough but can be a huge benefit to a project in the long term. \n\nAn important lesson to learn when it comes to optimising an R (or any) program \nis knowing both if to start and when to stop. You most likely want to optimize \nyour code because it is \"too slow\", but what that means will vary from \nproject to project. Be sure to consider what \"fast enough\" is for your project \nand how much needs to be optimized. If your program takes an hour to complete, \nspending 5 hours trying to make it faster can be time well spent if the script \nwill be run regularly, and a complete waste of time if it's an ad-hoc analysis.\n\nFor more information, see the CRAN Task View [High-Performance and Parallel \nComputing with R](\"https://CRAN.R-project.org/view=HighPerformanceComputing\").\nThe \"Performant Code\" section of Hadley Wickham's \n[Advanced R](\"http://adv-r.had.co.nz/\") is another great resource and provides \na deeper dive into what is covered in this guide. \n\n------\n\n# Update Your Installation\n\nOne of the easiest ways to improve the performance of R is to update R. In general, \nR will have a big annual release (i.e., 3.5.0) in the spring and around 3-4 \nsmaller patch releases (i.e., 3.5.1) throughout the rest of the year. If the \nmiddle digit of your installation is behind the current release, you should \nconsider updating.\n\nFor instance, R 3.5.0 implemented an improved read from text files. A 5GB file \ntook over 5 minutes to read in 3.4.4:\n\n![](optimization/images/data-load-3-4.png){width=75%}\n\nWhile 3.5.0 took less than half the time: \n\n![](optimization/images/data-load-3-5.png){width=75%}\n\nTo see what the R-core development team is up to, check out \nthe [NEWS](\"https://cran.r-project.org/doc/manuals/r-devel/NEWS.html\") file \nfrom the R project.\n\n------\n\n# Profiling & Benchmarking\n\nIn order to efficiently optimize your code, you'll first need to know where \nit's running slowest. The `profvis` package provides a nice way of visualizing \nthe execution time and memory useage of your program. \n\n```{r profile-01}\nlibrary(profvis)\nlibrary(dplyr)\n\nprofvis({\n\tdiamonds <- read.csv(\"optimization/data/diamonds.csv\")\n\t\n\tdiamonds_by_cut <- diamonds %>%\n\t\tgroup_by(cut) %>%\n\t\tsummarise_if(is.numeric, mean)\n\n\twrite.csv(diamonds_by_cut, file = \"optimization/data/diamonds_by_cut.csv\")\t\n\n})\n\n```\n\nIn this toy example it looks like the `read.csv` function is the bottleneck, so \nwork on optimizing that first.\n\nOnce you find the bottleneck that needs to be optimized, it can be useful to \nbenchmark different potential solutions. The `microbenchmark` package can help \nyou choose between different options. Continuing with the simple example with \nthe `diamonds` dataset, compare the base `read.csv` function with `read_csv` \nfrom the `readr` package.\n\n```{r benchmark-01}\nlibrary(microbenchmark)\n\nmicrobenchmark(\n\tread.csv(\"optimization/data/diamonds.csv\"),\n\treadr::read_csv(\"optimization/data/diamonds.csv\")\n)\n```\n\nIn this case, `read_csv` is about twice as fast as the base R implementations.\n\n# Parallel Computing\n\nOften, time-intensive R code can be sped up by breaking the execution of \nthe job across additional cores of your computer. This is called parallel computing.\n\n## Learn `lapply`/`purrr::map`\n\nLearning the `lapply` (and variants) function from Base R or the `map` (and variants) function from the `purrr` package is the first step in learning to run R code in parallel. Once you understand how `lapply` and `map` work, running your code in parallel will be simple.\n\nSay you have a vector of numbers and want to find the square root of each one\n(ignore for now that `sqrt` is vectorized, which will be covered later). \nYou could write a for loop and iterate over each element of the vector:\n\n```{r apply-01}\nx <- c(1, 4, 9, 16)\n\nout <- vector(\"list\", length(x))\nfor (i in seq_along(x)) {\n\tout[[i]] <- sqrt(x[[i]])\n}\nunlist(out)\n\n```\n\nThe `lapply` function essentially handles the overhead of constructing a for \nloop for you. The syntax is:\n\n```{r apply-02, eval = FALSE}\nlapply(X, FUN, ...)\n```\n\n`lapply` will then take each element of `X` and apply the `FUN`ction to it. \nOur simple example then becomes:\n\n```{r apply-03}\nx <- c(1, 4, 9, 16)\nout <- lapply(x, sqrt)\nunlist(out)\n```\n\nThose working within the `tidyverse` may use `map` from the `purrr` package equivalently:\n\n```{r apply-04}\nlibrary(purrr)\nx <- c(1, 4, 9, 16)\nout <- map(x, sqrt)\nunlist(out)\n```\n\n## Motivating Example\n\nOnce you are comfortable with `lapply` and/or `map`, running the same code in \nparallel takes just an additional line of code.\n\nFor `lapply` users, the `future.apply` package contains an equivalent \n`future_lapply` function. Just be sure to call `plan(multiprocess)` beforehand,\nwhich will handle the back-end orchestration needed to run in parallel.\n\n```{r parallel-01}\n# install.packages(\"future.apply\")\nlibrary(future.apply)\nplan(multiprocess)\nout <- future_lapply(x, sqrt)\nunlist(out)\n```\n\nFor `purrr` users, the `furrr` (i.e., future purrr) package includes an \nequivalent `future_map` function:\n\n```{r parallel-02}\n# install.packages(\"furrr\")\nlibrary(furrr)\nplan(multiprocess)\ny <- future_map(x, sqrt)\nunlist(y)\n```\n\nHow much faster did this simple example run in parallel?\n\n```{r parallel-03}\nlibrary(future.apply)\nplan(multiprocess)\n\nx <- c(1, 4, 9, 16)\n\nmicrobenchmark::microbenchmark(\n\tsequential = lapply(x, sqrt),\n\tparallel = future_lapply(x, sqrt),\n\tunit = \"s\"\n)\n```\n\nParallelization was actually slower. In this case, the overhead of \nsetting the code to run in parallel far outweighed any performance gain. In \ngeneral, parallelization works well on long-running & compute intensive jobs. \n\n## A (somewhat) More Complex Example\n\nIn this example we'll use the `diamonds` dataset from `ggplot2` and perform a \nkmeans cluster. We'll use `lapply` to iterate the number of clusters from 2 to \n5:\n\n```{r kmeans-01}\ndf <- ggplot2::diamonds\ndf <- dplyr::select(df, -c(cut, color, clarity))\n\ncenters = 2:5\n\nsystem.time(\n\tlapply(centers, \n\t\t\t\t function(x) kmeans(df, centers = x, nstart = 500)\n\t\t\t\t )\n\t)\n```\n\nA now running the same code in parallel:\n\n```{r kmeans-02}\nlibrary(future.apply)\nplan(multiprocess)\n\nsystem.time(\n\tfuture_lapply(centers, \n\t\t\t\t\t\t\t\tfunction(x) kmeans(df, centers = x, nstart = 500)\n\t\t\t\t\t\t\t\t)\n\t)\n```\n\nWhile we didn't achieve perfect scaling, we still get a nice bump in execution \ntime.\n\n## Additional Packages\n\nFor the sake of ease and brevity, this guide focused on the `futures` framework \nfor parallelization. However, you should be aware that there are a number of \nother ways to parallelize your code.\n\n### The `parallel` Package \n\nThe `parallel` package is included in your base R installation. It includes \nanalogues of the various `apply` functions:\n\n* `parLapply`\n* `mclapply` - not available on Windows\n\nThese functions generally require more setup, especially on Windows machines.\n\n### The `doParallel` Package\n\nThe `doParallel` package builds off of `parallel` and is \nuseful for code that uses for loops instead of `lapply`. Like the parallel \npackage, it generally requires more setup, especially on Windows machines.\n\n### Machine Learning - `caret`\n\nFor those running machine learning models, the `caret` package can easily \nleverage `doParallel` to speed up the execution of multiple models. Lifting \nthe example from the package documentation:\n\n```{r caret-01, eval = FALSE}\nlibrary(doParallel)\ncl <- makePSOCKcluster(5) # number of cores to use\nregisterDoParallel(cl)\n\n## All subsequent models are then run in parallel\nmodel <- train(y ~ ., data = training, method = \"rf\")\n\n## When you are done:\nstopCluster(cl)\n```\n\n\nBe sure to check out the full \n[documentation](\"http://topepo.github.io/caret/parallel-processing.html\") \nfor more detail.\n\n------\n\n# Big Data\n\nAs data collection and storage becomes easier and cheaper, it is relatively \nsimple to obtain relatively large data files. An important point to keep in \nmind is that the size of your data will generally expand when it is read \nfrom a storage device into R. A general rule of thumb is that a file will take \nsomewhere around 3-4 times more space in memory than it does on disk.\n\nFor instance, compare the size of the `iris` data set when it is saved as a \n.csv file locally vs the size of the object when it is read in to an R session: \n\n\n```{r size-01, message = FALSE}\nfile.size(\"optimization/data/iris.csv\") / 1000\ndf <- readr::read_csv(\"optimization/data/iris.csv\")\npryr::object_size(df)\n```\n\nThis means that on a standard Urban Institute desktop, you may have issues \nreading in files that are larger than 4 GB. \n\n## Object Size\n\nThe type of your data can have a big impact on the size of your data frame \nwhen you are dealing with larger files. There are four main types of atomic \nvectors in R:\n\n1. `logical`\n2. `integer`\n3. `double` (also called `numeric`)\n4. `character`\n\nEach of these data types occupies a different amount of space in memory - \n`logical` and `integer` vectors use 4 bytes per element, while a `double` will \noccupy 8 bytes. R uses a global string pool, so `character` vectors are hard \nto estimate, but will generally take up more space for element.\n\nConsider the following example:\n\n```{r size-02}\nx <- 1:100\npryr::object_size(x)\npryr::object_size(as.double(x))\npryr::object_size(as.character(x))\n```\n\nAn incorrect data type can easily cost you a lot of space in memory, especially \nat scale. This often happens when reading data from a text or csv file - data \nmay have a format such as `c(1.0, 2.0, 3.0)` and will be read in as a `numeric` \ncolumn, when `integer` is more appropriate and compact.\n\nYou may also be familiar with `factor` variables within R. Essentially a \n`factor` will represent your data as integers, and map them back to their \ncharacter representation. This can save memory when you have a compact and \nunique level of factors:\n\n```{r size-03}\nx <- sample(letters, 10000, replace = TRUE)\npryr::object_size(as.character(x))\npryr::object_size(as.factor(x))\n```\n\nHowever if each element is unique, or if there is not a lot of overlap among \nelements, than the overhead will make a factor larger than its character\nrepresentation:\n\n```{r size-04}\npryr::object_size(as.factor(letters))\npryr::object_size(as.character(letters))\n```\n\n## Cloud Computing\n\nSometimes, you will have data that are simply too large to ever fit on your \nlocal desktop machine. If that is the case, then the Elastic Cloud Computing \nEnvironment from the Office of Technology and Data Science can provide you with \neasy access to powerful analytic tools for computationally intensive project.\n\nThe Elastic Cloud Computing Environment allows researchers to quickly spin-up \nan Amazon Web Services (AWS) Elastic Cloud Compute (EC2) instance. These \ninstances offer increased memory to read in large datasets, along with \nadditional CPUs to provide the ability to process data in parallel at an \nimpressive scale.\n\n| Instance | CPU | Memory (GB) |\n|----------|-----|--------|\n| Desktop | 8 | 16 |\n| c5.4xlarge | 16 | 32 |\n| c5.9xlarge | 36 | 72 |\n| c5.18xlarge | 72 | 144 |\n| x1e.8xlarge | 32 | 976 |\n| x1e.16xlarge | 64 | 1952 |\n\nFeel free to contact Kyle Ueyama (kueyama@urban.org) if this would be useful \nfor your project.\n\n------\n\n# Common Pitfalls\n\n## For Loops and Vector Allocation\n\nA refrain you will often hear is that for loops in R are slow and need to be \navoided at all costs. This is not true! Rather, an improperly constructed loop \nin R can bring the execution of your program to a near standstill. \n\nA common for loop structure may look something like:\n\n```{r loop-01, eval = FALSE}\nx <- 1:100\nout <- c()\nfor (i in x) {\n\tout <- c(out, sqrt(x))\n\t}\n```\n\nThe bottleneck in this loop is with the allocation of the vector `out`. Every \ntime we iterate over an item in `x` and append it to `out`, R makes a copy \nof all the items already in `out`. As the size of the loop grows, your code \nwill take longer and longer to run.\n\nA better practice is to pre-allocate `out` to be the correct length, and then \ninsert the results as the loop runs.\n\n```{r loop-03, eval = FALSE}\nx <- 1:100\nout <- rep(NA, length(x))\nfor (i in seq_along(x)) {\n\t\tout[i] <- sqrt(x[i])\n}\n```\n\nA quick benchmark shows how much more efficient a loop with a pre-allocated \nresults vector is:\n\n```{r loop-04}\nbad_loop <- function(x) {\n\tout <- c()\n\tfor (i in x) {\n\t\tout <- c(out, sqrt(x))\n\t}\n}\n\ngood_loop <- function(x) {\n\tout <- rep(NA, length(x))\n\tfor (i in seq_along(x)) {\n\t\tout[i] <- sqrt(x[i])\n\t}\n}\n\nx <- 1:100\nmicrobenchmark::microbenchmark(\n\tbad_loop(x),\n\tgood_loop(x)\n)\n```\n\nAnd note how performance of the \"bad\" loop degrades as the loop size grows.\n\n```{r loop-05}\ny <- 1:250\n\nmicrobenchmark::microbenchmark(\n\tbad_loop(y),\n\tgood_loop(y)\n)\n```\n\n## Vectorized Functions\n\nMany functions in R are vectorized, meaning they can accept an entire vector \n(and not just a single value) as input. The `sqrt` function from the \nprior examples is one:\n\n```{r vectorised-01}\nx <- c(1, 4, 9, 16)\nsqrt(x)\n```\n\nThis removes the need to use `lapply` or a for loop. Vectorized functions in \nR are generally written in a compiled language like C, C++, or FORTRAN, which \nmakes their implementation faster.\n\n```{r vectorised-02}\nx <- 1:100\nmicrobenchmark::microbenchmark(\n\tlapply(x, sqrt),\n\tsqrt(x)\n)\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"includes":{"in_header":"analytics.html"},"css":"styles.css","toc":true,"toc_float":true,"pandoc_args":"--tab-stop=2"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["analytics.html"],"css":["styles.css"],"toc":true,"output-file":"optimization.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.37"},"extensions":{"book":{"multiFile":true}}}}}