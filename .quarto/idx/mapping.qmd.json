{"title":"Introduction","markdown":{"yaml":{"output":{"html_document":{"includes":{"in_header":"analytics.html"},"css":"styles.css","code_folding":"show","toc":true,"toc_float":true,"pandoc_args":"--tab-stop=2"}},"editor_options":{"markdown":{"wrap":72}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n<link rel=\"stylesheet\" href=\"//fonts.googleapis.com/css?family=Lato\" />\n\n::: {#header}\n<img src=\"mapping/www/images/urban-institute-logo.png\" width=\"350\">\n:::\n\n```{r markdown-setup, include=FALSE}\nknitr::opts_chunk$set(fig.path = \"mapping/www/images/\")\nknitr::opts_chunk$set(message = FALSE)\nknitr::opts_chunk$set(echo = TRUE)\nknitr::opts_chunk$set(warning = FALSE)\n\noptions(scipen = 999)\n```\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(here)\nlibrary(sf)\n```\n\n\n------------------------------------------------------------------------\n\nThis guide will teach you the concepts and code you will need for\nmapping and geospatial analysis in R. **This is a long guide, so if you\nneed something specific, we encourage you to scroll to the appropriate\nsection using the Table of Contents on the left.** If you just want copy and \npasteable code to create different kinds of maps, head to the [`Map Gallery`](#map_gallery). \nNow let's start mapping!\n\n![](mapping/www/images/yay_maps.gif)\n\n## Geospatial Workflow\n\nThis picture below outlines what we think are the main steps in a\ngeospatial workflow. This guide will be split into sections describing\neach of the steps.\n\n`r knitr::include_graphics(here::here(\"mapping\", \"www\", \"images\", \"geospatial_workflow.png\"), dpi = 350)`\n\n## Should this be a map?\n\nThe [Urban Institute Data Visualization Style\nGuide](http://urbaninstitute.github.io/graphics-styleguide/) offers some\nblunt but useful suggestions for maps:\n\n> Just because you've got geographic data, doesn't mean that you have to\n> make a map. Many times, there are more efficient storyforms that will\n> get your point across more clearly. If your data shows a very clear\n> geographic trend or if the absolute location of a place or event\n> matters, maps might be the best approach, but sometimes the reflexive\n> impulse to map the data can make you forget that showing the data in\n> another form might answer other---and sometimes more\n> important---questions.\n\nSo we would encourage you to think critically before making a map.\n\n## Why map with R?\n\nR can have a steeper learning curve than point-and-click tools - like\nQGIS or ArcGIS - for geospatial analysis and mapping. But creating maps\nin R has many advantages including:\n\n1)  **Reproducibility**: By creating maps with R code, you can easily\n    share the outputs and the code that generated the output with\n    collaborators, allowing them to replicate your work and catch errors\n    easily.\n\n2)  **Iteration**: With point and click software like ArcGIS, making 50\n    maps would be 50 times the work/time. But using R, we can easily\n    make make many iterations of the same map with a few changes to the\n    code.\n\n3)  **Easy Updates**: Writing code provides a roadmap for others (and\n    future you!) to quickly update parts of the map as needed. Say for\n    example a collaborator wanted to change the legend colors of 50\n    state maps. With R, this is possible in just a few seconds!\n\n4)  **An Expansive ecosystem**: There are several R packages that make\n    it very easy to get spatial data, create static and interactive\n    maps, and perform spatial analyses. This feature rich package\n    ecosystem which all play nice together is frankly unmatched by other\n    programming languages and even point and click tools like QGIS and\n    ArcGIS. Some of these R packages include:\n\n    -   `sf`: For managing and analyzing spatial dataframes\n    -   `tigris`: For downloading in Census geographies\n    -   `ggplot2`: For making publication ready static maps\n    -   `urbnmapr`: For automatically adding Urban styling to static\n        maps\n    -   `mapview`: For making expxploratory interactive maps\n\n5)  **Cost**: Most point-and-click tools for geospatial analysis are\n    proprietary and expensive. R is free open-source software. The\n    software and most of its packages can be used for free by anyone for\n    almost any use case.\n\n## Helpful Learning Resources\n\nIn addition to this guide, you may want to look at these other helpful\nresources:\n\n-   The Urban Institute [mapping training\n    series](https://ui-research.github.io/urbn101-mapping/) (with video\n    lectures and notes)\n-   Chapters\n    [5](https://walker-data.com/census-r/census-geographic-data-and-applications-in-r.html),\n    [6](https://walker-data.com/census-r/mapping-census-data-with-r.html),\n    and\n    [7](https://walker-data.com/census-r/spatial-analysis-with-us-census-data.html)\n    from Kyle Walker's Analyzing US Census Data\n    [book](https://walker-data.com/census-r/index.html).\n-   Andrew Heiss' fantastic mapping\n    [guide](https://datavizm20.classes.andrewheiss.com/example/12-example/)\n-   All of the vignettes for the [`sf`\n    package](https://cran.r-project.org/web/packages/sf/sf.pdf)\n-   [Geocomputation with\n    R](https://geocompr.robinlovelace.net/index.html): A book by Robin\n    Lovelace and others\n-   UChicago's R Spatial Workshops:\n    <https://spatialanalysis.github.io/tutorials/>\n\n# Get Spatial Data {#get_spatial_data}\n\n------------------------------------------------------------------------\n\n## library(sf) {.tabset .tabset-pills}\n\n### The short version\n\n`library(sf)` stores geospatial data, which are\n<font color=\"red\">**points**</font> (a single longitude/latitude),\n<font color=\"blue\">**lines**</font> (a pair of connected points), or\n<font color=\"black\">**polygons**</font> (a collection of points which\nmake a polygon) in a `geometry` column within R dataframes\n\n`r knitr::include_graphics(here::here(\"mapping\", \"www\", \"images\", \"amtrak_points_lines_polygons.jpg\"), dpi = 600)`\n\nThis is what `sf` dataframe looks like in the console:\n\n```{r print-sf-dataframe}\ndc_parks <- st_read(\"mapping/data/dc_parks.geojson\", \n\t\t\t\t\t\t\t\t\t\tquiet = TRUE)\n\n# Print just the NAME and geometry column\ndc_parks %>%\n  select(NAME) %>%\n  head(2)\n```\n\n\n### The long version\n\nThe `sf` library is a key tool for reading in, managing, and working\nwith spatial data in R. `sf` stands for simple features (not San\nFrancisco you Bay Area folks) and denotes a way to describe the spatial\nattributes of real life objects. The R object you will be working with\nmost frequently for mapping is an `sf` dataframe. An `sf` dataframe is\nessentially a regular R dataframe, with a couple of extra features for\nuse in mapping. These extra features exclusive to `sf` dataframes\ninclude:\n\n-   sticky `geometry` columns\n-   attached coordinate reference systems\n-   some other spatial metadata\n\nThe most important of the above list is the sticky `geometry` column,\nwhich is a magical column that contains all of the geographic\ninformation for each row of data. Say for example you had a `sf`\ndataframe of all DC census tracts. Then the `geometry` column would\ncontain all of the geographic points used to define DC census tract\npolygons. The stickiness of this column means that no matter what data\nmunging/filtering you do, you will not be able to drop or delete the\n`geometry` column. Below is a graphic to help you understand this:\n\n`r knitr::include_graphics(here::here(\"mapping\", \"www\", \"images\", \"sf_sticky_geometry.png\"), dpi = 600)`\n\ncredits: @allisonhorst\n\nThis is what an `sf` dataframe looks like in the console:\n\n```{r print_sf}\n# Read in spatial data about DC parks from DC Open Data Portal\ndc_parks  <- st_read(\"https://opendata.arcgis.com/api/v3/datasets/287eaa2ecbff4d699762bbc6795ffdca_9/downloads/data?format=geojson&spatialRefId=4326\",\n\t\t\t\t\t\t\t\t\t\tquiet = TRUE)\n\n# dc_parks <- st_read(\"mapping/data/dc_parks.geojson\")\n\n# Select just a few columns for readability\ndc_parks <- dc_parks %>%\n  select(NAME, geometry)\n\n# Print to the console\ndc_parks\n```\n\nNote that there is some spatial metadata such as the `Geometry Type`,\n`Bounding Box`, and `CRS` which shows up as a header before the actual\ncontents of the dataframe.\n\nSince `sf` dataframes operate similarly to regular dataframes, we can\nuse all our familiar `tidyverse` functions for data wrangling, including\n`select`, `filter`, `rename`, `mutate`, `group_by` and `summarize`. The\n`sf` package also has many functions that provide easy ways to replicate\ncommon tasks done in other GIS software like spatial joins, clipping,\nand buffering. Almost all of the mapping and geospatial analysis methods\ndescribed in this guide rely on you having an `sf` dataframe. So let's\ntalk about how to get one!\n\n## Importing spatial data {.tabset .tabset-pills}\n\nGetting an `sf` dataframe is always the first step in the geospatial\nworkflow. Here's how to import spatial data for...\n\n### States and counties\n\nWe highly recommend using the `library(urbnmapr)` package, which was\ncreated by folks here at Urban to easily create state and county level\nmaps. The `get_urbn_map()` function in the package allows you to read in\nspatial data on states and counties, with options to include\nterritories. Importantly, it will also display AL and HI as insets on\nthe map in accordance with the Urban Institute Data Visualization Style\nGuide. For information on how to install `urbnmapr`, see the [GitHub\nrepository](https://github.com/UrbanInstitute/urbnmapr).\n\nBelow is an example of how you would use `urbnmapr` to get an `sf`\ndataframe of all the states or counties in the US.\n\n```{r urbnmapr-1, eval=FALSE}\nlibrary(urbnmapr)\n\n# Get state data\nstates <- get_urbn_map(\"states\", sf = TRUE)\n\n# Can also get county data\ncounties <- get_urbn_map(\"counties\", sf = TRUE)\n```\n\n### Other Census geographies\n\nUse the `library(tigris)` package, which allows you to easily download\nTIGER and other cartographic boundaries from the US Census Bureau. In\norder to automatically load in the boundaries as `sf` objects, run\n`r options(tigris_class = \"sf\")` once per R session.\n\n`library(tigris)` has all the standard census geographies, including\ncensus tracts, counties, CBSAs, ZCTAs, congressional districts, tribal\nareas, and more. It also includes other elements such as water, roads,\nand military bases.\n\nBy default, `libraray(tigris)` will download large very large and\ndetailed TIGER line boundary files. For thematic mapping, the smaller\ncartographic boundary files are a better choice, as they are clipped to\nthe shoreline, generalized, and therefore usually smaller in size\nwithout losing too much accuracy. To load cartographic boundaries, use\nthe `cb = TRUE` argument. If you are doing detailed geospatial analysis\nand need the most detailed shapefiles, then you should use the detailed\nTIGER line boundary files and set `cb = FALSE`.\n\nBelow is an example of how you would use `library(tigris)` to get a `sf`\ndataframe of all Census tracts in DC for 2019.\n\n```{r tigris-1, eval=FALSE}\nlibrary(tigris)\n\n# Only need to set once per script\noptions(tigris_class = \"sf\")\n\ndc_tracts <- tracts(\n  state = \"DC\",\n  cb = TRUE,\n  year = 2019\n)\n```\n\nUnlike `library(urbnmapr)`, different functions are used to get\ngeographic data for different geographic levels. For instance, the\n`blocks()` function will load census block group data, and the\n`tracts()` function will load tract data. Other functions include\n`block_groups()`, `zctas()` , and `core_based_statistical_areas()`. For\nthe full list of supported geographies and functions, see the [package\nvignette](https://cran.r-project.org/web/packages/tigris/tigris.pdf).\n\nFor folks interested in pulling in Census demographic information along\nwith Census geographies, we recommend checking out the sister package to\n`library(tigris)`: `library(tidycensus)`. That package allows you to\ndownload in Census variables and Census geographic data simultaneously.\n\n### Countries\n\nWe recommend using the `library(rnaturalearth)` package, which is\nsimilar to `library(tigris)` but allows you to download and use\nboundaries beyond the US. Instead of setting class to `sf` one time per\nsession as we did with `library(tigris)`, you must set the\n`returnclass = \"sf\"` argument each time you use a function from the\npackage. Below is an example of downloading in an `sf` dataframe of all\nthe countries in the world.\n\n```{r natural-earth, eval = FALSE}\n\nlibrary(rnaturalearth)\n\nworld <- ne_countries(returnclass = \"sf\")\n\nggplot() +\n  geom_sf(data = world, mapping = aes())\n```\n\n### Your own files\n\n#### Shapefiles/GeoJSONS\n\nShapefiles and GeoJSONs are 2 common spatial file formats you will found\nout in the wild. `library(sf)` has a function called `st_read` which\nallows you to easily read in these files as `sf` dataframes. The only\nrequired argument is `dsn` or data source name. This is the filepath of\nthe `.shp` file or the `.geojson` file on your local computer. For\ngeojsons, `dsn` can also be a URL.\n\nBelow is an example of reading in a shapefile of fire stations in DC\nwhich is stored in `mapping/data/shapefiles/`. Note that shapefiles are\nactually stored as 6+ different files inside a folder. You need to\nprovide the filepath to the file ending in `.shp`.\n\n```{r list f-ei}\nlibrary(sf)\n\n# Print out all files in the directory\nlist.files(\"mapping/data/shapefiles\")\n\n# Read in .shp file\ndc_firestations <- st_read(\n  dsn = \"mapping/data/shapefiles/Fire_Stations.shp\",\n  quiet = TRUE\n)\n```\n\nAnd now `dc_firestations` is an `sf` dataframe you can use for all your\nmapping needs! `st_read` supports reading in a wide variety of other\nspatial file formats, including geodatabases, KML files, and over 200\nothers. For an incomplete list, please see the this `sf`\n[vignette](https://r-spatial.github.io/sf/articles/sf2.html).\n\n#### CSVs or dataframes with lat/lons\n\nIf you have a CSV with geographic information stored in columns, you\nwill need to read in the CSV as a regular R dataframe and then convert\nto an `sf` dataframe. `library(sf)` contains the `st_as_sf()` function\nfor converting regular R dataframes into an `sf` dataframe. The two\narguments you must specify for this function are:\n\n-   `coords`: A length 2 vector with the names of the columns\n    corresponding to longitude and latitude (in that order!). For\n    example, `c(\"lon\", \"lat\")`.\n-   `crs`: The CRS (coordinate references system) for your\n    longitude/latitude coordinates. Remember you need to specify both\n    the\\\n    authority and the SRID code, for example (\"EPSG:4326\"). For more\n    information on finding and setting CRS codes, please see the [`CRS`](#crs) section.\n\nBelow is an example of reading in data from a CSV and converting it to\nan `sf` dataframe.\n\n```{r make-sf}\nlibrary(sf)\n\n# Read in dataset of state capitals which is stored as a csv\nstate_capitals <- read_csv(\"mapping/data/state-capitals.csv\")\n\nstate_capitals <- state_capitals %>%\n  # Specify names of the lon/lat columns in the CSV to use to make geometry col\n  st_as_sf(\n    coords = c(\"longitude\", \"latitude\"),\n    crs = 4326\n  )\n```\n\nOne common mistake is that before converting to an `sf` dataframe, you\nmust drop any rows that have `NA` values for latitude or longitude. If\nyour data contains `NA` values, then the `st_as_sf()` function will\nthrow an error.\n\n## Appending spatial info to your data\n\nOftentimes, the data you are working with will just have state or county\nidentifiers - like FIPS codes or state abbreviations - but will not\ncontain any geographic information. In this case, you must do the extra\nwork of downloading in the geographic data as an `sf` dataframe and then\njoining your non-spatial data to the spatial data. Generally this\ninvolves 3 steps:\n\n1)  Reading in your own data as a data frame\n2)  Reading in the geographic data as an `sf` dataframe\n3)  Using `left_join` to merge the geographic data with your own non\n    spatial data and create a new expanded `sf` dataframe\n\nLet's say we had a dataframe on CHIP enrollment by state with state\nabbreviations.\n\n```{r readin-chip-data}\n\n# read the state CHIP data\nchip_by_state <- read_csv(\"mapping/data/chip-enrollment.csv\") %>%\n  # clean column names so there are no random spaces/uppercase letters\n  janitor::clean_names()\n\n# print to the console\nchip_by_state %>% head()\n```\n\nIn order to convert this to an `sf` dataframe, we need to read in the\nspatial boundaries for each state and append it to our dataframe. Here\nis how we do that with `get_urbn_map()` and `left_join()` .\n\n```{r append-spatial-info, cache = FALSE}\nlibrary(urbnmapr)\n\n# read in state geographic data from urbnmapr\nstates <- get_urbn_map(map = \"states\", sf = TRUE)\n\n# left join state geographies to chip data\nchip_with_geographies <- states %>%\n  left_join(\n    chip_by_state,\n    # Specify join column, which are slightly differently named in states and chip\n    # respectively\n    by = c(\"state_abbv\" = \"state_abbreviation\")\n  )\n\nchip_with_geographies %>%\n  select(state_fips, state_abbv, chip_enrollment)\n```\n\n```{r append-state-pops, include = FALSE, eval = TRUE, echo = FALSE}\n# TODO: DELETE THIS\n\n# Read in data on state populations from 2010\nstate_pops <-\n  read_csv(\"https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv\",\n    # Set this to disable printing column info to console\n    col_types = cols()\n  ) %>%\n  filter(ages == \"total\", year == \"2010\") %>%\n  select(state_abbv = `state/region`, population)\n\nchip_with_geographies <- chip_with_geographies %>%\n  # Specify left_join from tidylog to print summary messages\n  tidylog::left_join(state_pops, by = \"state_abbv\") %>%\n  # Calculate the chip enrollment percentage and append as a column\n  mutate(chip_pct = chip_enrollment / population)\n```\n\n# Project\n\n## Coordinate Reference Systems {.tabset .tabset-pills #crs}\n\n### The short version\n\nJust watch [this\nvideo](https://www.youtube.com/watch?v=vVX-PrBRtTY%60) and know the\nfollowing:\n\n-   All spatial data has a CRS, which specifies how to identify a\n    location on earth.\n\n-   It's important that all spatial datasets you are working with be in\n    the same CRS. You can find the CRS with `st_crs()` and change the\n    CRS with `st_transform()`.\n\n-   The Urban Institute Style Guide requires the use of the Atlas Equal\n    Earth Projection (`\"ESRI:102003\"`) for national maps. For state and local\n    maps, use [this](https://github.com/veltman/d3-stateplane) handy\n    guide to find an appropriate State Plane projection.\n\n### The long version\n\nCoordinate reference systems (CRS) specify the 3d shape of the earth and\noptionally how we project that 3d shape onto a 2d surface. They are an\nimportant part of working with spatial data as you need to ensure that\nall the data you are working with are in the same CRS in order for\nspatial operations and maps to be accurate.\n\nCRS can be specified either by name (ie Maryland State Plane) or\n**S**patial **R**eference System **ID**entifier (SRID). THe SRID is a\nnumeric identifier that uniquely identifies a coordinate reference\nsystem. Generally when referring to an SRID, you need to refer to an\nauthority (ie the data source) and a unique ID. An example is\n`EPSG:26985` which refers to the Maryland State plane projection from\nthe EPSG, or `ESRI:102003` which refers to the Atlas Equal Area\nprojection from ESRI. Most CRS codes will be from the EPSG, and some\nfrom ESRI and others. A good resource for finding/validating CRS codes\nis [epsg.io](epsg.io).\n\nSidenote - EPSG stands for the now defunct European Petroleum Survey\nGroup. And while oil companies have generally been terrible for the\nearth, the one nice thing they did for the earth was to set up common\nstandards for coordinate reference systems.\n\nYou might be thinking well isn't the earth just a sphere? Why do we need\nall this complicated stuff? And the answer is well the earth is [kind\nof](https://oceanservice.noaa.gov/facts/earth-round.html) a sphere, but\nit's really more of a misshapen ellipsoid which is pudgier at the\nequator than at the poles. To visualize how coordinate reference systems\nwork, imagine that the earth is a (lumpy) orange. Now peel the skin off\nan orange and try to flatten it. There are many ways to do it, but all\nwill create\n[distortions](https://twitter.com/neilrkaye/status/1050740679008296967)\nof some kind. The CRS will give us the formula we've used to specify the\nshape of the orange (usually a sphere or ellipsoid of some kind) and\noptionally, specify how we flattened the orange into 2d.\n\nBroadly, there are two kinds of Coordinate Reference Systems:\n\n1)  [**Geographic coordinate\n    systems**](https://www.ibm.com/support/knowledgecenter/en/SSGU8G_12.1.0/com.ibm.spatial.doc/ids_spat_407.html)\n\n    -   (sometimes called unprojected coordinate systems)\n    -   Specifies a 3d shape for the earth\n    -   Uses a spheroid/ellipsoid to approximate shape of the earth\n    -   Usually use decimal degree units (ie latitude/longitude) to\n        identify locations on earth\n        `r knitr::include_graphics(here::here(\"mapping\", \"www\", \"images\", \"gcs_image.png\"), dpi = 150)`\n\n2)  [**Projected coordinate\n    systems**](https://mgimond.github.io/Spatial/chp09-0.html#projected-coordinate-systems)\n\n    -   Specifies a 3d shape for the earth + a 2d mapping\n\n        -   Is a geographic coordinate system + a *projection*\n\n            `r knitr::include_graphics(here::here(\"mapping\", \"www\", \"images\", \"projecting_xkcd.png\"), dpi = 150)`\n\n            credit:\n            [xkcd](https://imgs.xkcd.com/comics/projecting.png)\n\n        -   **projection**: mathematical formula used to convert a 3d\n            coordinate system to a 2d flat coordinate system\n\n        -   Many different kinds of projections, including Equal Area,\n            Equidistant, Conformal, etc\n\n        -   All projections distort the true shape of the earth in some\n            way, either in terms of shape, area, or angle. Required\n            [xkcd comic](https://xkcd.com/977/)\n\n        -   Usually use linear units (ie feet, meters) and therefore\n            useful for distance based spatial operations (ie creating\n            buffers)\n\n## Finding the CRS\n\nIf you are lucky, your data will have embedded CRS data that will be\nautomatically detected when the file is read in. This is usually the\ncase for GeoJSONS (`.geojson`) and shapefiles (`.shp`). When you use\n`st_read()` on these files, you should see the CRS displayed in the\nmetadata:\n\n```{r import-shpfile-crs example, echo = FALSE}\nknitr::include_graphics(here(\"mapping\", \"www\", \"images\", \"sf_crs_pic.png\"))\n```\n\nYou can also the `st_crs()` function to find the CRS. The CRS code is located at\nthe end in `ID[authority, SRID]`.\n\n```{r st_crs}\nst_crs(dc_firestations)\n```\n\nSometimes, the CRS will be blank or `NA` as the dataset did not\nspecify the CRS. In that case you **MUST find and set the CRS for your\ndata before proceeding** with analysis. Below are some good rules of\nthumb for finding out what the CRS for your data is:\n\n-   For geojsons, the CRS should always be `EPSG:4326` (or WGS 84). The\n    official geojson specification states that this is the only valid\n    CRS for geojsons, but in the wild, this may not be true 100% of the\n    time.\n-   For shapefiles, there should be a file that ends in `.proj` in the\n    same directory as the `.shp` file. This file contains the projection\n    information for that file and should be used automatically when\n    reading in shapefiles.\n-   For CSV's with latitude/longitude columns, the CRS is usually\n    `EPSG:4326` (or WGS 84).\n-   Look at the metadata and any accompanying documentation to see if\n    the coordinate reference system for the data is specified\n\nIf none of the above rules of thumb apply to you, check out the\n`crsuggest` R [package](https://github.com/walkerke/crsuggest).\n\nOnce you've identified the appropriate CRS, you can set the CRS for your\ndata with `st_crs()`:\n\n```{r set_crs, eval = FALSE}\n\n# If you are certain that your data contains coordinates in the ESRI Atlas Equal Earth projections\nst_crs(some_sf_dataframe) <- st_crs(\"ESRI:102003\")\n```\n\n## Transforming the CRS\n\nOften you will need to change the CRS for your `sf` dataframe so that\nall datasets you are using have the same CRS, or to use a projected CRS\nfor performing more accurate spatial operations. You can do this with\n`st_transform`:\n\n```{r transform-crs}\n# Transforming CRS from WGS 84 to Urban required Equal Earth Projection\nstate_capitals <- state_capitals %>% st_transform(\"ESRI:102003\")\n```\n\n`st_transform()` also allows you to just use the CRS of another `sf`\ndataframe when transforming.\n\n```{r transform-crs-with-another-sf-object}\n# transform CRS of chip_with_geographies to be the same as CRS of dc_firestations\nchip_with_geographies <- chip_with_geographies %>%\n  st_transform(crs = st_crs(state_capitals))\n```\n\nIf you are working with local data, you should use an appropriate state\nplane projection instead of the Atlas Equal Earth projection which is\nmeant for national maps. `library(crsuggest)` can simplify the process\nof picking an appropriate state plane CRS.\n\n```{r crsuggest-ex, cache = TRUE}\nlibrary(crsuggest)\n\nsuggest_crs(dc_firestations) %>%\n  # Use the value in the \"crs_code\" column to transform CRS's\n  head(4)\n```\n\n# Map\n\nIn order to start mapping, you need an `sf` dataframe. If you don't have\none, see the [`Get Spatial Data`](#get_spatial_data) section above.\n\n## The basics\n\n### library(ggplot2)\n\nMost mapping in R fits the same theoretical framework as plotting in R\nusing `library(ggplot2)`. To learn more about ggplot2, visit the Data\nViz\n[page](https://urbaninstitute.github.io/r-at-urban/graphics-guide.html#Grammar_of_Graphics_and_Conventions)\nor read the official ggplot [book](html).\n\nThe key function for mapping is **the special `geom_sf()` function** which works\nwith `sf` dataframes. This function magically detects whether you have\npoint or polygon spatial data and displays the results on a map.\n\n### A simple map\n\nTo make a simple map, add `geom_sf()` to a `ggplot()` and set\n`data = an_sf_dataframe`. Below is code for making a map of all 50\nstates using `library(urbnmapr)`:\n\n```{r first-map, cache = TRUE}\nlibrary(urbnmapr)\n\nstates <- get_urbn_map(\"states\", sf = TRUE)\n\nggplot() +\n  geom_sf(\n    data = states,\n    mapping = aes()\n  )\n```\n\n## Styling\n\n### `library(urbnthemes)`\n\n`library(urbnthemes)` automatically styles maps in accordance with the\n[Urban Institute Data Visualization Style\nGuide](http://urbaninstitute.github.io/graphics-styleguide/). By using\n`library(urbnthemes)`, you can create publication ready maps you can\nimmediately drop in to Urban research briefs or blog posts.\n\nTo install `urbnthemes`, visit the package's [GitHub\nrepository](https://github.com/UrbanInstitute/urbnthemes) and follow the\ninstructions. There are 2 ways to use the `urbnthemes` functions:\n\n```{r urbnthemes}\n\nlibrary(urbnthemes)\n\n# You can either run this once per script to automatically style all maps with\n# the Urban theme\nset_urbn_defaults(style = \"map\")\n\n# Or you can add `+ theme_urbn_map()` to the end of every map you make\nggplot() +\n  geom_sf(states, mapping = aes()) +\n  theme_urbn_map()\n```\n\n### Layering\n\nYou can layer multiple points/lines/polygons on top of each other using\nthe `+` operator from `library(ggplot2)`. The shapes will appear from\nbottom to top (ie the last mapped object will show up on top). It is\nimportant that all layers are in the same CRS (coordinate reference\nsystem).\n\n```{r layers, cache = TRUE}\n\nstate_capitals <- state_capitals %>%\n  # This will change CRS to ESRI:102003 and shift the AK and HI state capitals\n  # point locations to the appropriate locations on the inset maps.\n  tigris::shift_geometry() %>%\n  # For now filter out AL and HI as their state capitals will be slightly off.\n  filter(!state %in% c(\"Alaska\", \"Hawaii\"))\n\nggplot() +\n  geom_sf(\n    data = states,\n    mapping = aes()\n  ) +\n  # Note we change the data argument\n  geom_sf(\n    data = state_capitals,\n    mapping = aes(),\n    # urbnthemes library has urbn color palettes built in.\n    color = palette_urbn_main[\"yellow\"],\n    size = 2.0\n  ) +\n  theme_urbn_map()\n```\n\n### Fill and Outline Colors\n\nThe same commands used to change colors, opacity, lines, size, etc. in\ncharts can be used for maps too. To change the colors of the map , just\nuse the `fill =` and `color =` parameters in `geom_sf()`. `fill` will\nchange the fill color of polygons; `color` will change the color of\npolygon outlines, lines, and points.\n\nGenerally, maps that show the magnitude of a variable use the blue\nsequential ramp and maps that display positives and negatives use the\ndiverging color ramp.`library(urbnthemes)` contains inbuilt. helper\nvariables (like `palette_urbn_main`) for accessing color palettes from\nthe Urban Data Viz Style guide. If for example you want states to be\nUrban's magenta color:\n\n```{r urbnthemes- pink}\n\nggplot() +\n  geom_sf(states,\n    mapping = aes(),\n    # Adjust polygon fill color\n    fill = palette_urbn_main[\"magenta\"],\n    # Adjust polygon outline color\n    color = \"white\"\n  ) +\n  theme_urbn_map()\n```\n\n### Adding text\n\nYou can also add text, like state abbreviations, directly to your map\nusing `geom_sf_text` and the helper function `get_urbn_labels()`.\n\n```{r geom_sf_text}\nlibrary(urbnmapr)\n\nggplot() +\n  geom_sf(states,\n    mapping = aes(),\n    color = \"white\"\n  ) +\n  theme_urbn_map() +\n  # Generates dataframe of state abbv and appropriate location to plot them\n  geom_sf_text(\n    data = get_urbn_labels(\n      map = \"states\",\n      sf = TRUE\n    ),\n    aes(label = state_abbv),\n    size = 3\n  )\n```\n\nThere's also `geom_sf_label()` if you want labels with a border.\n\n# Map Gallery {#map_gallery}\n\nBelow are copy and pasteable examples of maps you can make, after you\nhave an `sf` dataframe.\n\n## Choropleth Maps\n\nChoropleth maps display geographic areas with shades, colors, or\npatterns in proportion to a variable or variables. Choropleth maps can\nrepresent massive geographies like the entire world and small\ngeographies like Census Tracts. To make a choropleth map, you need to\nset `geom_sf(aes(fill = some_variable_name))`. Below are examples\n\n### Continuous color scale\n\n```{r choropoleth_continious}\n# Map of CHIP enrollment percentage by state\nchip_with_geographies_map <- chip_with_geographies %>%\n  ggplot() +\n  geom_sf(aes(\n    # Color in states by the chip_pct variable\n    fill = chip_pct\n  ))\n\n\n# Below add-ons to the map are optional, but make the map look prettier.\nchip_with_geographies_map +\n  # scale_fill_gradientn adds colors with more interpolation and reverses color scale\n  scale_fill_gradientn(\n    # Convert legend from decimal to percentages\n    labels = scales::percent_format(),\n    # Make legend title more readable\n    name = \"CHIP Enrollment %\",\n    # Manually add 0 to lower limit to include it in legend. NA=use maximum value in data\n    limits = c(0, NA),\n    # Set number of breaks on legend = 3\n    n.breaks = 3\n  )\n```\n\n### Discrete color scale\n\nThe quick and dirty way is with `scale_fill_steps()`, which creates\ndiscretized bins for continuous variables:\n\n```{r chorpleth_disccrete}\nchip_with_geographies %>%\n  ggplot() +\n  geom_sf(aes(\n    # Color in states by the chip_pct variable\n    fill = chip_pct\n  )) +\n  scale_fill_steps(\n    # Convert legend from decimal to percentages\n    labels = scales::percent_format(),\n    # Make legend title more readable\n    name = \"CHIP Enrollment %\",\n    # Show top and bottom limits on legend\n    show.limits = TRUE,\n    # Roughly set number of bins. Won't be exact as R uses algorithms under the\n    # hood for pretty looking breaks.\n    n.breaks = 4\n  )\n```\n\nOften you will want to manually generate the bins yourself to give you\nmore fine grained control over the exact legend text. (ie `1% - 1.8%`,\n`1.8 - 2.5%`, etc). Below is an example of discretizing the continuous\n`chip_pct` variable yourself using `cut_interval()` and a helper\nfunction to get nice looking interval labels:\n\n```{r format_intervals}\n\n# Helper function to clean up R generated intervals into nice looking interval labels\nformat_interval <- function(interval_text) {\n  text <- interval_text %>%\n    # Remove open and close brackets which is R generated math notation\n    str_remove_all(\"\\\\(\") %>%\n    str_remove_all(\"\\\\)\") %>%\n    str_remove_all(\"\\\\[\") %>%\n    str_remove_all(\"\\\\]\") %>%\n    str_replace_all(\",\", \" — \")\n\n  # Convert decimal ranges to percent ranges\n  text <- text %>%\n    str_split(\" — \") %>%\n    map(~ as.numeric(.x) %>%\n      scales::percent() %>%\n      paste0(collapse = \" — \")) %>%\n    unlist() %>%\n    # By default character vectors are plotted in alphabetical order. We want\n    # factors in reverse alphabetical order to get correct colors in ggplot\n    fct_rev()\n\n  return(text)\n}\n\nchip_with_geographies <- chip_with_geographies %>%\n  # cut_interval into n groups with equal range. Set boundary so 0 is included in the bins\n  mutate(chip_pct_interval = cut_interval(chip_pct, n = 5)) %>%\n  # Generate nice looking interval labels\n  mutate(chip_pct_interval = format_interval(chip_pct_interval))\n```\n\nAnd now we can map the discretized `chip_pct_interval` variable using\n`geom_sf()`:\n\n```{r make_discrete_map}\nchip_with_geographies %>%\n  ggplot() +\n  geom_sf(aes(\n    # Color in states by the chip_pct variable\n    fill = chip_pct_interval\n  )) +\n  # Default is to use main urban palette, which assumes unrelated groups. We\n  # adjust colors manually to be on Urban cyan palette\n  scale_fill_manual(\n    values = palette_urbn_cyan[c(8, 7, 5, 3, 1)],\n    name = \"CHIP Enrollment %\"\n  )\n```\n\nIn addition to `cut_interval` there are [similar\nfunctions](https://ggplot2.tidyverse.org/reference/cut_interval.html)\nfor creating intervals/bins with slightly different rules. When creating\nbins, be careful as changing the number of bins can drastically change\nhow the map looks.\n\n## Bubble Maps\n\nThis is just a layered map with one polygon layer and one point layer,\nwhere the points are sized in accordance with a variable in your data.\n\n```{r bubble_maps, cache = TRUE}\nset_urbn_defaults(style = \"map\")\n\n# Get sf dataframe of DC tracts\nlibrary(tigris)\ndc_tracts <- tracts(\n  state = \"DC\",\n  year = 2019,\n  progress_bar = FALSE\n)\n\n# Add bubbles for firestations\nggplot() +\n  geom_sf(data = dc_tracts, fill = palette_urbn_main[\"gray\"]) +\n  geom_sf(\n    data = dc_firestations,\n    # Size bubbles by number of trucks at each station\n    aes(size = TRUCK),\n    color = palette_urbn_main[\"yellow\"],\n    # Adjust transparency for readability\n    alpha = 0.8\n  )\n```\n\n## Dot-density Maps\n\nThese maps scatter dots within a geographic area. Typically each dot\nrepresents a unit (like 100 people, or 1000 houses). To create this kind\nof map, you need to start with an `sf` dataframe that is of `geometry`\ntype `POLYGON` or `MULTIPOLYGON` and then sample points within the\npolygon.\n\nThe below code generates a dot-density map representing people of\ndifferent races within Washington DC tracts The code may look a little\ncomplicated, but the key workhorse function is `st_sample()` which\nsamples points within each polygon to use in the dot density map:\n\n```{r dot_density_maps, cache = TRUE}\nlibrary(tidycensus)\n\n# Get counts by race of DC tracts\ndc_pop <- get_acs(\n  geography = \"tract\",\n  state = \"DC\",\n  year = 2019,\n  variables = c(\n    Hispanic = \"DP05_0071\",\n    White = \"DP05_0077\",\n    Black = \"DP05_0078\",\n    Asian = \"DP05_0080\"\n  ),\n  geometry = TRUE,\n  progress_bar = FALSE\n)\n\n# Get unique groups (ie races)\ngroups <- unique(dc_pop$variable)\n\n# For each unique group (ie race), generate sampled points\ndc_race_dots <- map_dfr(groups, ~ {\n  dc_pop %>%\n    # .x = the group used in the loop\n    filter(variable == .x) %>%\n    # Use the projected MD state plane for accuracy\n    st_transform(crs = \"EPSG:6487\") %>%\n    # Have every dot represent 100 people\n    mutate(est100 = as.integer(estimate / 100)) %>%\n    st_sample(size = .$est100, exact = TRUE) %>%\n    st_sf() %>%\n    # Add group (ie race) as a column so we can use it when plotting\n    mutate(group = .x)\n})\n\n\nggplot() +\n  # Plot tracts, then dots on top of tracts\n  geom_sf(\n    data = dc_pop,\n    # Make interior of tracts transparent and boundaries black\n    fill = \"transparent\",\n    color = \"black\"\n  ) +\n  geom_sf(\n    data = dc_race_dots,\n    # Color in dots by racial group\n    aes(color = group),\n    # Adjust transparency and size to be more readable\n    alpha = 0.5,\n    size = 1.1,\n    stroke = FALSE\n  )\n```\n\n## Geofacets\n\nGeofaceting arranges sub-geography-specific plots into a grid that\nresembles a larger geography (usually the US). This can be a useful\nalternative to choropleth maps, which tend to overemphasize\nlow-population density areas with large areas. To make geofacetted\ncharts, use the `facet_geo()` function from the `geofacet` library,\nwhich can be thought of as equivalent to ggplot2's `facet_wrap()`. For\nthis example, we'll use the built-in `state_ranks` data.\n\n```{r geofacet-data}\nlibrary(geofacet)\n\nhead(state_ranks %>% as_tibble())\n```\n\n```{r geofacet-ex, cache = TRUE}\nset_urbn_defaults(style = \"print\")\n\nstate_ranks %>%\n  filter(variable %in% c(\"education\", \"employment\")) %>%\n  ggplot(aes(x = rank, y = variable)) +\n  geom_col() +\n  facet_geo(\n    facets = \"state\",\n    # Use custom urban geofacet grid which is built into urbnthemes\n    # For now we need to rename a few columns as urbnthemes has to be\n    # updated\n    grid = urbnthemes::urbn_geofacet %>%\n      rename(\n        code = state_code,\n        name = state_name\n      )\n  )\n```\n\nInteractive geofacets of the United States have been used in Urban\nFeatures like [A Matter of\nTime](https://apps.urban.org/features/long-prison-terms/trends.html)\nwhich included geofaceted line charts showing trends in incarceration by\nstate. Static geofacets of the United States were included in [Barriers\nto Accessing Homeownership Down Payment, Credit, and\nAffordability](https://www.urban.org/sites/default/files/publication/94801/barriers-to-homeownership-down-payments-credit-access-and-affordability_3.pdf)\nby the Housing Finance Policy Center.\n\n## Cartograms\n\nCartograms are a modified form of a choropleth map with intentionally\ndistorted sizes that map to a variable in your data. Below we create a\ncartogram with `library(cartogram)` where the state sizes are\nproportional to the population.\n\n```{r cartogram-example, cache = TRUE}\nlibrary(cartogram)\n\nset_urbn_defaults(style = \"map\")\n\nchip_with_geographies_weighted <- chip_with_geographies %>%\n  # Note column name needs to be in quotes for this package\n  cartogram_cont(weight = \"population\")\n\nggplot() +\n  geom_sf(\n    data = chip_with_geographies_weighted,\n    # Color in states by chip percentages\n    aes(fill = chip_pct)\n  )\n```\n\n## Interactive Maps\n\nInteractive maps can be a great exploratory tool to explore and\nunderstand your data. And luckily there are a lot of new R packages that\nmake it really easy to create them. Interactive maps are powerful but\n**we do not recommend them for official use in Urban publications** as\ngetting them in Urban styles and appropriate basemaps can be tricky\n(reach out to\n[anarayanan\\@urban.org](mailto:anarayanan@urban.org){.email} if you\nreally want to include them).\n\n### `library(mapview)`\n\n`library(mapview)` is probably the most user friendly of the interactive\nmapping R libraries. All you have to do to create an interactive map is:\n\n```{r show-mapview}\nlibrary(mapview)\n\n\nchip_with_geographies_for_interactive_mapping <- chip_with_geographies %>%\n  # Filter out AL and HI bc they would appear in Mexico. If you want AL, HI and\n  # in the correct place in interactive maps, make sure to use tigris::states()\n  filter(!state_abbv %in% c(\"AK\", \"HI\"))\n\nmapview(chip_with_geographies_for_interactive_mapping)\n```\n\nWhen you click on an object, you get a popup table of all it's attributes. And \nwhen you hover over an object, you get a popup with an object id.\n\nEach of the above behaviors can be changed if desired. As you'll see in\nthe below section, the syntax for `library(mapview)` is significantly\ndifferent from `library(ggplot2)` so be careful!\n\n#### Coloring in points/polygons\n\nIn order to create a choropleth map where we color in the\npoints/polygons by a variable, we need to feed in a column name *in\nquotes* to the`zcol` argument inside the `mapview()` function:\n\n```{r mapview_zcol}\n# Create interactive state map colored in by chip enrollment\nmapview(chip_with_geographies_for_interactive_mapping, zcol = \"chip_enrollment\")\n```\n\nIf you want more granular control over the color palette for the legend\ncan also feed in a vector of color hex codes to `col.regions` along with\na column name to `zcol`. This will create a continuous color range along\nthe provided colors. Be careful though as the color interpolation is not\nperfect.\n\n```{r mapview-colors-granular}\n# library(RColorBrewer)\nmapview(chip_with_geographies_for_interactive_mapping,\n  col.regions = c(\n    palette_urbn_green[6],\n    \"white\",\n    palette_urbn_cyan[6]\n  ),\n  zcol = \"chip_enrollment\"\n)\n```\n\nIf you want to color in all points/polygons as the same color, just feed\nin a single color hex code to the `col.regions` argument:\n\n```{r mapview-colors}\nmapview(chip_with_geographies_for_interactive_mapping,\n  col.regions = palette_urbn_green[5]\n)\n```\n\n#### Adding layers\n\nYou can add multiple `sf` objects on the same map by using the `+`\noperator. This is very useful when comparing 2 or more spatial datasets.\n\n```{r mapview-layers}\nmapview(chip_with_geographies_for_interactive_mapping, col.regions = palette_urbn_green[5]) +\n  mapview(state_capitals, col.regions = palette_urbn_cyan[5])\n```\n\nYou can even create slider maps by using the `|` operator!\n\n```{r mapview-sliders}\nmapview(chip_with_geographies_for_interactive_mapping, col.regions = palette_urbn_green[5]) |\n  mapview(state_capitals, col.regions = palette_urbn_cyan[5])\n```\n\n### More details\n\nTo learn more about more advanced options with `mapview` maps, check out\nthe\n[documentation](https://r-spatial.github.io/mapview/articles/articles/mapview_02-advanced.html)\npage and the [reference\nmanual](https://cran.r-project.org/web/packages/mapview/mapview.pdf).\n\nThere are also other interactive map making packages in R like `leaflet`\n(which `mapview` is a more user friendly wrapper of), `tmap`, and\n`mapdeck`. To learn about these other packages, [this book\nchapter](https://geocompr.robinlovelace.net/adv-map.html#interactive-maps)\nis a good starting point.\n\n# Spatial Operations\n\n\n## Cropping\n\nCropping (or clipping) is geographically filtering an `sf` dataframe to just \nthe area we are interested in. Say we wanted to look at the roads around Fire \nStation 24 in DC. \n\n```{r roads_cropping_before, cache = TRUE}\nlibrary(tigris)\nlibrary(units)\n\ndc_firestations <- dc_firestations %>%\n  st_transform(\"EPSG:6487\")\n\n\n# Draw 500 meter circle around one fire station\nfire_station_24_buffered <- dc_firestations %>%\n  filter(NAME == \"Engine 24 Station\") %>%\n  st_buffer(set_units(500, \"meter\"))\n\n# Get listing of all roads in DC\ndc_roads <- roads(\n  state = \"DC\",\n  county = \"District of Columbia\",\n  class = \"sf\",\n  progress_bar = FALSE\n) %>%\n  st_transform(\"EPSG:6487\")\n\n# View roads on top of fire_station\nggplot() +\n  # Order matters! We need to plot fire_stations first, and then roads on top\n  # to see overlapping firestations\n  geom_sf(\n    data = fire_station_24_buffered,\n    fill = palette_urbn_cyan[1],\n    color = palette_urbn_cyan[7]\n  ) +\n  geom_sf(\n    data = dc_roads,\n    color = palette_urbn_gray[7]\n  ) +\n  theme_urbn_map()\n```\n\nWe can clip the larger roads dataframe to just roads that overlap with the circle \naround the fire station with `st_intersection()`.\n\n```{r roads_cropping_after}\n\n# Use st_intersection() to crop the roads data to just roads within the\n# fire_station radius\ndc_roads_around_fire_station_24_buffered <- fire_station_24_buffered %>%\n  st_intersection(dc_roads)\n\nggplot() +\n  geom_sf(\n    data = fire_station_24_buffered,\n    fill = palette_urbn_cyan[1],\n    color = palette_urbn_cyan[7]\n  ) +\n  geom_sf(\n    data = dc_roads_around_fire_station_24_buffered,\n    color = palette_urbn_gray[7]\n  ) +\n  theme_urbn_map()\n```\n\n\n**More Coming Soon!**\n\n## Calculating Distance\n\n## Spatial Joins\n\n### Point to Polygon\n\n### Polygon to Polygon\n## Aggregating\n\n## Drive/Transit times\n\n## Geocoding\n\nGeocoding is the process of turning text (usually addresses) into\ngeographic coordinates (usually latitudes/longitudes) for use in\nmapping. For Urban researchers, we highly recommend using the [Urban\ngeocoder](https://tech-tools.urban.org/geocoding/) as it is fast,\naccurate, designed to work with sensitive/confidential data and most\nimportantly free to use for Urban researchers! To learn about how we set\nup and chose the geocoder for the Urban Institute, you can read our\n[Data\\@Urban\nblog](https://medium.com/@urban_institute/choosing-a-geocoder-for-the-urban-institute-86192f656c5f).\n\n### Cleaning Addresses\n\nThe single most important factor in getting accurate geocoded data is\nhaving cleaned, well structured address data. This can prove difficult as\naddress data out in the wild is often messy and unstandardized. While\nthe rules for cleaning addresses are very data specific, below are some\nexamples of clean addresses you should aim for in your data cleaning\nprocess:\n\n```{r cleaned-addr, cache=TRUE,eval=TRUE,results=TRUE, echo=FALSE}\nlibrary(gt)\ncleaned_address_table <- tribble(\n  ~\"f_address\", ~\"Type of address\",\n  \"123 Troy Drive, Pillowtown, CO, 92432\", \"residnetial address\",\n  \"789 Abed Avenue, Apt 666, Blankesburg, CO, 92489\", \"residential apartment address\",\n  \"Shirley Boulevard and Britta Drive, Blanketsburg, CO, 92489\", \"street intersection\",\n  \"Pillowtown, CO\", \"city\",\n  \"92489, CO\", \"Zip Code\",\n)\n\ngt(cleaned_address_table) %>%\n  # tab_header(title = md(\"Clean Address Examples\")) %>%\n  opt_row_striping(row_striping = TRUE) %>%\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_column_labels(\n      columns = vars(f_address, `Type of address`)\n    )\n  ) %>%\n  opt_align_table_header(align = c(\"left\")) %>%\n  tab_options(\n    container.width = \"100%\",\n    container.height = \"400px\",\n    # column_labels.background.color  = palette_urbn_cyan[1],\n    table.border.top.width = 0,\n    table.border.bottom.width = 0,\n    column_labels.border.bottom.width = 0,\n  )\n```\n\nAll that being said, our geocoder is pretty tolerant of different\naddress formats, typos/spelling errors and missing states, zip codes,\netc. So don't spend too much time cleaning every address in the data.\nAlso note that while our geocoder is able to geocode cities and zip\ncodes, it will return the lat/lon of the center of the city/zip code,\nwhich may not be what you want.\n\n### Instructions\n\nTo use the [Urban geocoder](https://tech-tools.urban.org/geocoding/),\nyou will need to:\n\n1)  Generate a CSV with a column named `f_address` which contains the\n    addresses in single line format (ie\n    `123 Abed Avenue, Blanketsburg, CO, 94328`). This means that if you\n    have the addresses split across multiple columns (ie `Address`,\n    `City`, `State`, `Zip` columns), you will need to concatenate them\n    into one column. Also see our Address cleaning section above.\n\n2)  Go to the Urban geocoder and answer the initial questions. This will\n    tell you whether your data is non-confidential or confidential data,\n    and allow you to upload your CSV for geocoding.\n\n3)  Wait for an email telling you your results are ready. If your data\n    is non-confidential, this email will contain a link to your geocoded\n    results. This link expires in 24 hours, so make sure to download\n    your data before then. If you data is confidential, the email will\n    contain a link to the location on the Y Drive where your\n    confidential geocoded data is stored. You can specify this output\n    folder when submitting the CSV in step 1.\n\n### Geocoder outputs\n\n<p>\n\nThe geocoded file will be your original data, plus a few more columns\n(including latitude and longitude). each of the new columns that have\nbeen appended to your original data. [It's very important that you take\na look at the Addr_type\ncolumn]{style=\"background-color: #FFFF00;  font-weight: bold\"} in the\nCSV before doing further analysis to check the accuracy of the geocoding\nprocess.\n\n</p>\n\n+------------+------------------------------------------------------+\n| Column     | Description                                          |\n+:===========+:=====================================================+\n| Match_addr | The actual address that the inputted address was     |\n|            | matched to. This is the address that the geocoder    |\n|            | used to get Latitudes / Longitudes. If there are     |\n|            | potentially many typos or non standard address       |\n|            | formats in your data file, you will want to take a   |\n|            | close look at this column to confirm that the        |\n|            | matched address correctly handled typos and badly    |\n|            | formatted addresses.                                 |\n+------------+------------------------------------------------------+\n| Longitude  | The WGS 84 datum Longitude (EPSG code 4326)          |\n|            |                                                      |\n+------------+------------------------------------------------------+\n| Latitude   | The WGS 84 datum Latitude (EPSG code 4326)           |\n+------------+------------------------------------------------------+\n| Addr_type  | The match level for a geocode request. This should   |\n|            | be used as an indicator of the precision of geocode  |\n|            | results. Generally, Subaddress, PointAddress,        |\n|            | StreetAddress, and StreetInt represent accurate      |\n|            | matches. The list below contains all possible values |\n|            | for this field. **Green values represent High        |\n|            | accuracy matches, yellow represents Medium accuracy  |\n|            | matches and red represents Low accuracy/inaccurate   |\n|            | matches**. If you have many yellow and red values in |\n|            | your data, you should manually check the results     |\n|            | before proceeding with analysis. All possible        |\n|            | values:\\                                             |\n|            | \\                                                    |\n|            | **Subaddress:** A subset of a PointAddress that      |\n|            | represents a house or building subaddress location,  |\n|            | such as an apartment unit, floor, or individual      |\n|            | building within a complex. The UnitName, UnitType,   |\n|            | LevelName, LevelType, BldgName, and BldgType field   |\n|            | values help to distinguish subaddresses which may be |\n|            | associated with the same PointAddress. Reference     |\n|            | data consists of point features with associated      |\n|            | house number, street name, and subaddress elements,  |\n|            | along with administrative divisions and optional     |\n|            | postal code; for example, 3836 Emerald Ave, Suite C, |\n|            | La Verne, CA, 91750.\\                                |\n|            | \\                                                    |\n|            | **PointAddress:** A street address based on points   |\n|            | that represent house and building locations.         |\n|            | Typically, this is the most spatially accurate match |\n|            | level. Reference data contains address points with   |\n|            | associated house numbers and street names, along     |\n|            | with administrative divisions and optional postal    |\n|            | code. The X / Y (`Longitude`/`Latitude`) and         |\n|            | `geometry` output values for a PointAddress match    |\n|            | represent the street entry location for the address; |\n|            | this is the location used for routing operations.    |\n|            | The `DisplayX` and `DisplayY` values represent the   |\n|            | rooftop, or actual, location of the address.         |\n|            | Example: 380 New York St, Redlands, CA, 92373.\\      |\n|            | \\                                                    |\n|            | **StreetAddress** --- A street address that differs  |\n|            | from PointAddress because the house number is        |\n|            | interpolated from a range of numbers. Reference data |\n|            | contains street center lines with house number       |\n|            | ranges, along with administrative divisions and      |\n|            | optional postal code information, for example, 647   |\n|            | Haight St, San Francisco, CA, 94117.\\                |\n|            | \\                                                    |\n|            | **StreetInt:** A street address consisting of a      |\n|            | street intersection along with city and optional     |\n|            | state and postal code information. This is derived   |\n|            | from StreetAddress reference data, for example,      |\n|            | Redlands Blvd & New York St, Redlands, CA, 92373.\\   |\n|            | \\                                                    |\n|            | **StreetName:** Similar to a street address but      |\n|            | without the house number. Reference data contains    |\n|            | street centerlines with associated street names (no  |\n|            | numbered address ranges), along with administrative  |\n|            | divisions and optional postal code, for example, W   |\n|            | Olive Ave, Redlands, CA, 92373.\\                     |\n|            | \\                                                    |\n|            | **StreetAddressExt:** An interpolated street address |\n|            | match that is returned when parameter                |\n|            | matchOutOfRange=true and the input house number      |\n|            | exceeds the house number range for the matched       |\n|            | street segment.\\                                     |\n|            | \\                                                    |\n|            | **DistanceMarker:** A street address that represents |\n|            | the linear distance along a street, typically in     |\n|            | kilometers or miles, from a designated origin        |\n|            | location. Example: Carr 682 KM 4, Barceloneta,       |\n|            | 00617.\\                                              |\n|            | \\                                                    |\n|            | **PostalExt:** A postal code with an additional      |\n|            | extension, such as the United States Postal Service  |\n|            | ZIP+4. Reference data is postal code points with     |\n|            | extensions, for example, 90210-3841.\\                |\n|            | \\                                                    |\n|            | **POI:** ---Points of interest. Reference data       |\n|            | consists of administrative division place-names,     |\n|            | businesses, landmarks, and geographic features, for  |\n|            | example, Golden Gate Bridge.\\                        |\n|            | \\                                                    |\n|            | **Locality:** A place-name representing a populated  |\n|            | place. The Type output field provides more detailed  |\n|            | information about the type of populated place.       |\n|            | Possible Type values for Locality matches include    |\n|            | Block, Sector, Neighborhood, District, City,         |\n|            | MetroArea, County, State or Province, Territory,     |\n|            | Country, and Zone. Example: Bogotá, COL,\\            |\n|            | \\                                                    |\n|            | **PostalLoc:** A combination of postal code and city |\n|            | name. Reference data is typically a union of postal  |\n|            | boundaries and administrative (locality) boundaries, |\n|            | for example, 7132 Frauenkirchen.\\                    |\n|            | \\                                                    |\n|            | **Postal:** Postal code. Reference data is postal    |\n|            | code points, for example, 90210 USA.                 |\n+------------+------------------------------------------------------+\n| Score      | A number from 1--100 indicating the degree to which  |\n|            | the input tokens in a geocoding request match the    |\n|            | address components in a candidate record. A score of |\n|            | 100 represents a perfect match, while lower scores   |\n|            | represent decreasing match accuracy.                 |\n+------------+------------------------------------------------------+\n| Status     | Indicates whether a batch geocode request results in |\n|            | a match, tie, or unmatched. Possible values include\\ |\n|            | \\                                                    |\n|            | M - Match. The returned address matches the input    |\n|            | address and is the highest scoring candidate.\\       |\n|            | \\                                                    |\n|            | T - Tied. The returned address matches the input     |\n|            | address but has the same score as one or more        |\n|            | additional candidates.\\                              |\n|            | \\                                                    |\n|            | U - Unmatched. No addresses match the inputted        |\n|            | address.                                             |\n+------------+------------------------------------------------------+\n| geometry   | The WKT (Well-known text) representation of the      |\n|            | latitudes and longitudes. This column may be useful  |\n|            | if you're reading the CSV into R, Python, or ArcGIS  |\n+------------+------------------------------------------------------+\n| Region     | The state that `Match_addr` is located in            |\n+------------+------------------------------------------------------+\n| RegionAbbr | Abbreviated State Name. For example, CA for          |\n|            | California                                           |\n+------------+------------------------------------------------------+\n| Subregion  | The county that the input address is located in      |\n+------------+------------------------------------------------------+\n| MetroArea  | The name of the Metropolitan area that `Match_addr`  |\n|            | is located in. This field may be blank if the input  |\n|            | address is not located within a metro area.          |\n+------------+------------------------------------------------------+\n| City       | The city that `Match_addr` is located in             |\n+------------+------------------------------------------------------+\n| Nbrhd      | The Neighborhood that `Match_addr` is located in.    |\n|            | Note these are ESRI defined neighborhoods which may  |\n|            | or may not align with other sources neighborhood     |\n|            | definitions                                          |\n+------------+------------------------------------------------------+\n\n\\\n\n# Geospatial Modeling\n\nComing soon!\n\n# Bibliography and references\n\n------------------------------------------------------------------------\n\n```{r session-info}\n\nsessionInfo()\n```\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"includes":{"in_header":"analytics.html"},"css":"styles.css","code_folding":"show","toc":true,"toc_float":true,"pandoc_args":"--tab-stop=2"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["analytics.html"],"css":["styles.css"],"toc":true,"output-file":"mapping.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.37","editor_options":{"markdown":{"wrap":72}}},"extensions":{"book":{"multiFile":true}}}}}